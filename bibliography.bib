
@book{schuld_machine_2021,
	location = {Cham},
	title = {Machine Learning with Quantum Computers},
	rights = {https://www.springer.com/tdm},
	isbn = {978-3-030-83097-7 978-3-030-83098-4},
	url = {https://link.springer.com/10.1007/978-3-030-83098-4},
	series = {Quantum Science and Technology},
	publisher = {Springer International Publishing},
	author = {Schuld, Maria and Petruccione, Francesco},
	urldate = {2024-03-28},
	date = {2021},
	langid = {english},
	doi = {10.1007/978-3-030-83098-4},
}

@online{nielsen_quantum_2010,
	title = {Quantum Computation and Quantum Information: 10th Anniversary Edition},
	url = {https://www.cambridge.org/highereducation/books/quantum-computation-and-quantum-information/01E10196D0A682A6AEFFEA52D53BE9AE},
	shorttitle = {Quantum Computation and Quantum Information},
	abstract = {One of the most cited books in physics of all time, Quantum Computation and Quantum Information remains the best textbook in this exciting field of science. This 10th anniversary edition includes an introduction from the authors setting the work in context. This comprehensive textbook describes such remarkable effects as fast quantum algorithms, quantum teleportation, quantum cryptography and quantum error-correction. Quantum mechanics and computer science are introduced before moving on to describe what a quantum computer is, how it can be used to solve problems faster than 'classical' computers and its real-world implementation. It concludes with an in-depth treatment of quantum information. Containing a wealth of figures and exercises, this well-known textbook is ideal for courses on the subject, and will interest beginning graduate students and researchers in physics, computer science, mathematics, and electrical engineering.},
	titleaddon = {Higher Education from Cambridge University Press},
	author = {Nielsen, Michael A. and Chuang, Isaac L.},
	urldate = {2024-03-28},
	date = {2010-12-09},
	langid = {english},
	doi = {10.1017/CBO9780511976667},
	note = {{ISBN}: 9780511976667
Publisher: Cambridge University Press},
	file = {Snapshot:C\:\\Users\\erick\\Zotero\\storage\\UKZ5DK7N\\01E10196D0A682A6AEFFEA52D53BE9AE.html:text/html},
}

@article{hallgren_polynomial-time_2007,
	title = {Polynomial-time quantum algorithms for Pell's equation and the principal ideal problem},
	volume = {54},
	issn = {0004-5411, 1557-735X},
	url = {https://dl.acm.org/doi/10.1145/1206035.1206039},
	doi = {10.1145/1206035.1206039},
	abstract = {We give polynomial-time quantum algorithms for three problems from computational algebraic number theory. The ﬁrst is Pell’s equation. Given a positive nonsquare integer d, Pell’s equation is x2 − dy2 = 1 and the goal is to ﬁnd its integer solutions. Factoring integers reduces to ﬁnding integer solutions of Pell’s equation, but a reduction in the other direction is not known and appears more difﬁcult. The second problem we solve is the principal ideal problem in real quadratic number ﬁelds. This problem, which is at least as hard as solving Pell’s equation, is the one-way function underlying the Buchmann–Williams key exchange system, which is therefore broken by our quantum algorithm. Finally, assuming the generalized Riemann hypothesis, this algorithm can be used to compute the class group of a real quadratic number ﬁeld.},
	pages = {1--19},
	number = {1},
	journaltitle = {Journal of the {ACM}},
	shortjournal = {J. {ACM}},
	author = {Hallgren, Sean},
	urldate = {2024-03-28},
	date = {2007-03},
	langid = {english},
}

@article{van_dam_quantum_2006,
	title = {Quantum Algorithms for Some Hidden Shift Problems},
	volume = {36},
	issn = {0097-5397, 1095-7111},
	url = {http://epubs.siam.org/doi/10.1137/S009753970343141X},
	doi = {10.1137/S009753970343141X},
	abstract = {Almost all of the most successful quantum algorithms discovered to date exploit the ability of the Fourier transform to recover subgroup structures of functions, especially periodicity. The fact that Fourier transforms can also be used to capture shift structure has received far less attention in the context of quantum computation. In this paper, we present three examples of “unknown shift” problems that can be solved eﬃciently on a quantum computer using the quantum Fourier transform. For one of these problems, the shifted Legendre symbol problem, we give evidence that the problem is hard to solve classically, by showing a reduction from breaking algebraically homomorphic cryptosystems. We also deﬁne the hidden coset problem, which generalizes the hidden shift problem and the hidden subgroup problem. This framework provides a uniﬁed way of viewing the ability of the Fourier transform to capture subgroup and shift structure.},
	pages = {763--778},
	number = {3},
	journaltitle = {{SIAM} Journal on Computing},
	shortjournal = {{SIAM} J. Comput.},
	author = {Van Dam, Wim and Hallgren, Sean and Ip, Lawrence},
	urldate = {2024-03-28},
	date = {2006-01},
	langid = {english},
}

@article{shor_polynomial-time_1997,
	title = {Polynomial-Time Algorithms for Prime Factorization and Discrete Logarithms on a Quantum Computer},
	volume = {26},
	issn = {0097-5397},
	url = {https://epubs.siam.org/doi/10.1137/S0097539795293172},
	doi = {10.1137/S0097539795293172},
	abstract = {Recently a great deal of attention has been focused on quantum computation following a sequence of results [Bernstein and Vazirani, in Proc. 25th Annual {ACM} Symposium Theory Comput., 1993, pp. 11--20, {SIAM} J. Comput., 26 (1997), pp. 1277--1339], [Simon, in Proc. 35th Annual {IEEE} Symposium Foundations Comput. Sci., 1994, pp. 116--123, {SIAM} J. Comput., 26 (1997), pp. 1340--1349], [Shor, in Proc. 35th Annual {IEEE} Symposium Foundations Comput. Sci., 1994, pp. 124--134] suggesting that quantum computers are more powerful than classical probabilistic computers. Following Shor's result that factoring and the extraction of discrete logarithms are both solvable in quantum polynomial time, it is natural to ask whether all of \${\textbackslash}{NP}\$ can be efficiently solved in quantum polynomial time. In this paper, we address this question by proving that relative to an oracle chosen uniformly at random with probability 1 the class \${\textbackslash}{NP}\$ cannot be solved on a quantum Turing machine ({QTM}) in time \$o(2{\textasciicircum}\{n/2\})\$. We also show that relative to a permutation oracle chosen uniformly at random with probability 1 the class \${\textbackslash}{NP} {\textbackslash}cap {\textbackslash}{coNP}\$ cannot be solved on a {QTM} in time \$o(2{\textasciicircum}\{n/3\})\$. The former bound is tight since recent work of Grover [in \{{\textbackslash}it Proc.{\textbackslash} \$28\$th Annual {ACM} Symposium Theory Comput.\}, 1996] shows how to accept the class \${\textbackslash}{NP}\$ relative to any oracle on a quantum computer in time \$O(2{\textasciicircum}\{n/2\})\$.},
	pages = {1484--1509},
	number = {5},
	journaltitle = {{SIAM} Journal on Computing},
	shortjournal = {{SIAM} J. Comput.},
	author = {Shor, Peter W.},
	urldate = {2024-03-28},
	date = {1997-10},
	note = {Publisher: Society for Industrial and Applied Mathematics},
}

@article{ciliberto_quantum_2018,
	title = {Quantum machine learning: a classical perspective},
	volume = {474},
	issn = {1364-5021, 1471-2946},
	url = {https://royalsocietypublishing.org/doi/10.1098/rspa.2017.0551},
	doi = {10.1098/rspa.2017.0551},
	shorttitle = {Quantum machine learning},
	abstract = {Recently, increased computational power and data availability, as well as algorithmic advances, have led machine learning ({ML}) techniques to impressive results in regression, classification, data generation and reinforcement learning tasks. Despite these successes, the proximity to the physical limits of chip fabrication alongside the increasing size of datasets is motivating a growing number of researchers to explore the possibility of harnessing the power of quantum computation to speed up classical {ML} algorithms. Here we review the literature in quantum {ML} and discuss perspectives for a mixed readership of classical {ML} and quantum computation experts. Particular emphasis will be placed on clarifying the limitations of quantum algorithms, how they compare with their best classical counterparts and why quantum resources are expected to provide advantages for learning problems. Learning in the presence of noise and certain computationally hard problems in {ML} are identified as promising directions for the field. Practical questions, such as how to upload classical data into quantum form, will also be addressed.},
	pages = {20170551},
	number = {2209},
	journaltitle = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	shortjournal = {Proc. R. Soc. A.},
	author = {Ciliberto, Carlo and Herbster, Mark and Ialongo, Alessandro Davide and Pontil, Massimiliano and Rocchetto, Andrea and Severini, Simone and Wossnig, Leonard},
	urldate = {2024-03-28},
	date = {2018-01},
	langid = {english},
}

@article{preskill_quantum_2018,
	title = {Quantum Computing in the {NISQ} era and beyond},
	volume = {2},
	issn = {2521-327X},
	url = {http://arxiv.org/abs/1801.00862},
	doi = {10.22331/q-2018-08-06-79},
	abstract = {Noisy Intermediate-Scale Quantum ({NISQ}) technology will be available in the near future. Quantum computers with 50-100 qubits may be able to perform tasks which surpass the capabilities of today's classical digital computers, but noise in quantum gates will limit the size of quantum circuits that can be executed reliably. {NISQ} devices will be useful tools for exploring many-body quantum physics, and may have other useful applications, but the 100-qubit quantum computer will not change the world right away --- we should regard it as a significant step toward the more powerful quantum technologies of the future. Quantum technologists should continue to strive for more accurate quantum gates and, eventually, fully fault-tolerant quantum computing.},
	pages = {79},
	journaltitle = {Quantum},
	shortjournal = {Quantum},
	author = {Preskill, John},
	urldate = {2024-04-03},
	date = {2018-08-06},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1801.00862 [cond-mat, physics:quant-ph]},
	keywords = {Condensed Matter - Strongly Correlated Electrons, Quantum Physics},
}

@misc{szegedy_intriguing_2014,
	title = {Intriguing properties of neural networks},
	url = {http://arxiv.org/abs/1312.6199},
	abstract = {Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties.},
	number = {{arXiv}:1312.6199},
	publisher = {{arXiv}},
	author = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
	urldate = {2024-04-03},
	date = {2014-02-19},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1312.6199 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@misc{goodfellow_explaining_2015,
	title = {Explaining and Harnessing Adversarial Examples},
	url = {http://arxiv.org/abs/1412.6572},
	abstract = {Several machine learning models, including neural networks, consistently misclassify adversarial examples—inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high conﬁdence. Early attempts at explaining this phenomenon focused on nonlinearity and overﬁtting. We argue instead that the primary cause of neural networks’ vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the ﬁrst explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the {MNIST} dataset.},
	number = {{arXiv}:1412.6572},
	publisher = {{arXiv}},
	author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
	urldate = {2024-04-03},
	date = {2015-03-20},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1412.6572 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{kurakin_adversarial_2017,
	title = {Adversarial Machine Learning at Scale},
	url = {http://arxiv.org/abs/1611.01236},
	abstract = {Adversarial examples are malicious inputs designed to fool machine learning models. They often transfer from one model to another, allowing attackers to mount black box attacks without knowledge of the target model’s parameters. Adversarial training is the process of explicitly training a model on adversarial examples, in order to make it more robust to attack or to reduce its test error on clean inputs. So far, adversarial training has primarily been applied to small problems. In this research, we apply adversarial training to {ImageNet} (Russakovsky et al., 2014). Our contributions include: (1) recommendations for how to succesfully scale adversarial training to large models and datasets, (2) the observation that adversarial training confers robustness to single-step attack methods, (3) the ﬁnding that multi-step attack methods are somewhat less transferable than singlestep attack methods, so single-step attacks are the best for mounting black-box attacks, and (4) resolution of a “label leaking” effect that causes adversarially trained models to perform better on adversarial examples than on clean examples, because the adversarial example construction process uses the true label and the model can learn to exploit regularities in the construction process.},
	number = {{arXiv}:1611.01236},
	publisher = {{arXiv}},
	author = {Kurakin, Alexey and Goodfellow, Ian and Bengio, Samy},
	urldate = {2024-04-03},
	date = {2017-02-10},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1611.01236 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Cryptography and Security},
}

@misc{papernot_transferability_2016,
	title = {Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples},
	url = {http://arxiv.org/abs/1605.07277},
	shorttitle = {Transferability in Machine Learning},
	abstract = {Many machine learning models are vulnerable to adversarial examples: inputs that are specially crafted to cause a machine learning model to produce an incorrect output. Adversarial examples that aﬀect one model often aﬀect another model, even if the two models have diﬀerent architectures or were trained on diﬀerent training sets, so long as both models were trained to perform the same task. An attacker may therefore train their own substitute model, craft adversarial examples against the substitute, and transfer them to a victim model, with very little information about the victim. Recent work has further developed a technique that uses the victim model as an oracle to label a synthetic training set for the substitute, so the attacker need not even collect a training set to mount the attack. We extend these recent techniques using reservoir sampling to greatly enhance the eﬃciency of the training procedure for the substitute model. We introduce new transferability attacks between previously unexplored (substitute, victim) pairs of machine learning model classes, most notably {SVMs} and decision trees. We demonstrate our attacks on two commercial machine learning classiﬁcation systems from Amazon (96.19\% misclassiﬁcation rate) and Google (88.94\%) using only 800 queries of the victim model, thereby showing that existing machine learning approaches are in general vulnerable to systematic black-box attacks regardless of their structure.},
	number = {{arXiv}:1605.07277},
	publisher = {{arXiv}},
	author = {Papernot, Nicolas and {McDaniel}, Patrick and Goodfellow, Ian},
	urldate = {2024-04-03},
	date = {2016-05-23},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1605.07277 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Cryptography and Security},
}

@inproceedings{carlini_towards_2017,
	location = {San Jose, {CA}, {USA}},
	title = {Towards Evaluating the Robustness of Neural Networks},
	isbn = {978-1-5090-5533-3},
	url = {http://ieeexplore.ieee.org/document/7958570/},
	doi = {10.1109/SP.2017.49},
	abstract = {Neural networks provide state-of-the-art results for most machine learning tasks. Unfortunately, neural networks are vulnerable to adversarial examples: given an input x and any target classiﬁcation t, it is possible to ﬁnd a new input x that is similar to x but classiﬁed as t. This makes it difﬁcult to apply neural networks in security-critical areas. Defensive distillation is a recently proposed approach that can take an arbitrary neural network, and increase its robustness, reducing the success rate of current attacks’ ability to ﬁnd adversarial examples from 95\% to 0.5\%.},
	eventtitle = {2017 {IEEE} Symposium on Security and Privacy ({SP})},
	pages = {39--57},
	booktitle = {2017 {IEEE} Symposium on Security and Privacy ({SP})},
	publisher = {{IEEE}},
	author = {Carlini, Nicholas and Wagner, David},
	urldate = {2024-04-03},
	date = {2017-05},
	langid = {english},
}

@misc{madry_towards_2019,
	title = {Towards Deep Learning Models Resistant to Adversarial Attacks},
	url = {http://arxiv.org/abs/1706.06083},
	abstract = {Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at https://github.com/{MadryLab}/mnist\_challenge and https://github.com/{MadryLab}/cifar10\_challenge.},
	number = {{arXiv}:1706.06083},
	publisher = {{arXiv}},
	author = {Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
	urldate = {2024-04-03},
	date = {2019-09-04},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1706.06083 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@misc{bommasani_opportunities_2022,
	title = {On the Opportunities and Risks of Foundation Models},
	url = {http://arxiv.org/abs/2108.07258},
	abstract = {{AI} is undergoing a paradigm shift with the rise of models (e.g., {BERT}, {DALL}-E, {GPT}-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.},
	number = {{arXiv}:2108.07258},
	publisher = {{arXiv}},
	author = {Bommasani, Rishi and Hudson, Drew A. and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S. and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and Brynjolfsson, Erik and Buch, Shyamal and Card, Dallas and Castellon, Rodrigo and Chatterji, Niladri and Chen, Annie and Creel, Kathleen and Davis, Jared Quincy and Demszky, Dora and Donahue, Chris and Doumbouya, Moussa and Durmus, Esin and Ermon, Stefano and Etchemendy, John and Ethayarajh, Kawin and Fei-Fei, Li and Finn, Chelsea and Gale, Trevor and Gillespie, Lauren and Goel, Karan and Goodman, Noah and Grossman, Shelby and Guha, Neel and Hashimoto, Tatsunori and Henderson, Peter and Hewitt, John and Ho, Daniel E. and Hong, Jenny and Hsu, Kyle and Huang, Jing and Icard, Thomas and Jain, Saahil and Jurafsky, Dan and Kalluri, Pratyusha and Karamcheti, Siddharth and Keeling, Geoff and Khani, Fereshte and Khattab, Omar and Koh, Pang Wei and Krass, Mark and Krishna, Ranjay and Kuditipudi, Rohith and Kumar, Ananya and Ladhak, Faisal and Lee, Mina and Lee, Tony and Leskovec, Jure and Levent, Isabelle and Li, Xiang Lisa and Li, Xuechen and Ma, Tengyu and Malik, Ali and Manning, Christopher D. and Mirchandani, Suvir and Mitchell, Eric and Munyikwa, Zanele and Nair, Suraj and Narayan, Avanika and Narayanan, Deepak and Newman, Ben and Nie, Allen and Niebles, Juan Carlos and Nilforoshan, Hamed and Nyarko, Julian and Ogut, Giray and Orr, Laurel and Papadimitriou, Isabel and Park, Joon Sung and Piech, Chris and Portelance, Eva and Potts, Christopher and Raghunathan, Aditi and Reich, Rob and Ren, Hongyu and Rong, Frieda and Roohani, Yusuf and Ruiz, Camilo and Ryan, Jack and Ré, Christopher and Sadigh, Dorsa and Sagawa, Shiori and Santhanam, Keshav and Shih, Andy and Srinivasan, Krishnan and Tamkin, Alex and Taori, Rohan and Thomas, Armin W. and Tramèr, Florian and Wang, Rose E. and Wang, William and Wu, Bohan and Wu, Jiajun and Wu, Yuhuai and Xie, Sang Michael and Yasunaga, Michihiro and You, Jiaxuan and Zaharia, Matei and Zhang, Michael and Zhang, Tianyi and Zhang, Xikun and Zhang, Yuhui and Zheng, Lucia and Zhou, Kaitlyn and Liang, Percy},
	urldate = {2024-04-03},
	date = {2022-07-12},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2108.07258 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computers and Society},
}

@incollection{shor_quantum_nodate,
	title = {Quantum Computing},
	pages = {467--486},
	booktitle = {Documenta Mathematica - Extra Volume {ICM} 1998},
	author = {Shor, Peter W.},
}

@book{national_academies_of_sciences_engineering_and_medicine_quantum_2019,
	title = {Quantum Computing: Progress and Prospects},
	isbn = {978-0-309-47972-1},
	shorttitle = {Quantum Computing},
	abstract = {Quantum mechanics, the subfield of physics that describes the behavior of very small (quantum) particles, provides the basis for a new paradigm of computing. First proposed in the 1980s as a way to improve computational modeling of quantum systems, the field of quantum computing has recently garnered significant attention due to progress in building small-scale devices. However, significant technical advances will be required before a large-scale, practical quantum computer can be achieved. Quantum Computing: Progress and Prospects provides an introduction to the field, including the unique characteristics and constraints of the technology, and assesses the feasibility and implications of creating a functional quantum computer capable of addressing real-world problems. This report considers hardware and software requirements, quantum algorithms, drivers of advances in quantum computing and quantum devices, benchmarks associated with relevant use cases, the time and resources required, and how to assess the probability of success.},
	pagetotal = {273},
	publisher = {National Academies Press},
	author = {National Academies of Sciences, Engineering, \{and\} Medicine},
	date = {2019-03-27},
	langid = {english},
	note = {Google-Books-{ID}: {ATH}3DwAAQBAJ},
	keywords = {Computers / General, Computers / Information Technology},
}

@article{rivest_method_1978,
	title = {A method for obtaining digital signatures and public-key cryptosystems},
	volume = {21},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/359340.359342},
	doi = {10.1145/359340.359342},
	abstract = {An encryption method is presented with the novel property that publicly revealing an encryption key does not thereby reveal the corresponding decryption key. This has two important consequences: (1) Couriers or other secure means are not needed to transmit keys, since a message can be enciphered using an encryption key publicly revealed by the intented recipient. Only he can decipher the message, since only he knows the corresponding decryption key. (2) A message can be “signed” using a privately held decryption key. Anyone can verify this signature using the corresponding publicly revealed encryption key. Signatures cannot be forged, and a signer cannot later deny the validity of his signature. This has obvious applications in “electronic mail” and “electronic funds transfer” systems. A message is encrypted by representing it as a number M, raising M to a publicly specified power e, and then taking the remainder when the result is divided by the publicly specified product,
              n
              , of two large secret primer numbers p and q. Decryption is similar; only a different, secret, power d is used, where e * d ≡ 1(mod (p - 1) * (q - 1)). The security of the system rests in part on the difficulty of factoring the published divisor,
              n
              .},
	pages = {120--126},
	number = {2},
	journaltitle = {Communications of the {ACM}},
	shortjournal = {Commun. {ACM}},
	author = {Rivest, R. L. and Shamir, A. and Adleman, L.},
	urldate = {2024-04-04},
	date = {1978-02},
	langid = {english},
}

@article{schumacher_quantum_1995,
	title = {Quantum Coding},
	volume = {51},
	rights = {http://link.aps.org/licenses/aps-default-license},
	issn = {1050-2947, 1094-1622},
	url = {https://link.aps.org/doi/10.1103/PhysRevA.51.2738},
	doi = {10.1103/PhysRevA.51.2738},
	pages = {2738--2747},
	number = {4},
	journaltitle = {Physical Review A},
	shortjournal = {Phys. Rev. A},
	author = {Schumacher, Benjamin},
	urldate = {2024-04-05},
	date = {1995-04-01},
	langid = {english},
}

@article{dirac_new_1939,
	title = {A New Notation for Quantum Mechanics},
	volume = {35},
	issn = {1469-8064, 0305-0041},
	url = {https://www.cambridge.org/core/journals/mathematical-proceedings-of-the-cambridge-philosophical-society/article/new-notation-for-quantum-mechanics/4631DB9213D680D6332BA11799D76AFB},
	doi = {10.1017/S0305004100021162},
	abstract = {In mathematical theories the question of notation, while not of primary importance, is yet worthy of careful consideration, since a good notation can be of great value in helping the development of a theory, by making it easy to write down those quantities or combinations of quantities that are important, and difficult or impossible to write down those that are unimportant. The summation convention in tensor analysis is an example, illustrating how specially appropriate a notation can be.},
	pages = {416--418},
	number = {3},
	journaltitle = {Mathematical Proceedings of the Cambridge Philosophical Society},
	author = {Dirac, P. a. M.},
	urldate = {2024-04-08},
	date = {1939-07},
	langid = {english},
}

@article{born_quantenmechanik_1926,
	title = {Quantenmechanik der Stoßvorgänge},
	volume = {38},
	issn = {0044-3328},
	url = {https://doi.org/10.1007/BF01397184},
	doi = {10.1007/BF01397184},
	abstract = {Die Schrödingersche Form der Quantenmechanik erlaubt in natürlicher Weise die Häufigkeit eines Zustandes zu definieren mit Hilfe der Intensität der zugeordneten Eigenschwingung. Diese Auffassung führt zu einer Theorie der Stoß-vorgänge, bei der die Übergangswahrscheinlichkeiten durch das asymptotische Verhalten aperiodischer Lösungen bestimmt werden.},
	pages = {803--827},
	number = {11},
	journaltitle = {Zeitschrift für Physik},
	shortjournal = {Z. Physik},
	author = {Born, Max},
	urldate = {2024-04-08},
	date = {1926-11-01},
	langid = {german},
}

@article{bloch_nuclear_1946,
	title = {Nuclear Induction},
	volume = {70},
	rights = {http://link.aps.org/licenses/aps-default-license},
	issn = {0031-899X},
	url = {https://link.aps.org/doi/10.1103/PhysRev.70.460},
	doi = {10.1103/PhysRev.70.460},
	pages = {460--474},
	number = {7},
	journaltitle = {Physical Review},
	shortjournal = {Phys. Rev.},
	author = {Bloch, F.},
	urldate = {2024-04-08},
	date = {1946-10-01},
	langid = {english},
}
