\chapter{Implementation}\label{chapter:implementation} \

In Chapter~\ref{chapter:implementation} we will introduce
the methods used to implement the experiments to answer
the research goals. In Section~\ref{section:vqa_training}
we will present how the \ac{qml} models are trained.
Futhermore, in Section~\ref{section:vqa_attacks} the
attack methodology on the \ac{vqa} model will be described. \

\section{Variational Quantum Algorithm Model Training}\label{section:vqa_training} \

In this section we will specify how the \ac{vqa} models
were trained. In Subsection~\ref{subsection:preprocess},
we will describe the dataset preprocessing methodology required
for the training. Later on in Subsection~\ref{subsection:config},
the configuration files utilized by the training pipeline will
be defined. Afterwards, in Subsection~\ref{subsection:noise_injection}
we will present how the noise is injected to the
\ac{qml} models during training and evaluation.
Finally, the developed pipeline for the training will
be presented in Subsection~\ref{subsection:pipeline}. \

\subsection{Datasets Preprocessing}\label{subsection:preprocess} \

In order to reduce the execution time of the model training, all the
datasets mentioned in the Section~\ref{section:datasets} were
stored preprocessed. All the datasets but the Plus-Minus dataset
were retrieved using OpenML~\cite{vanschoren_openml_2014}. We
obtained the Plus-Minus dataset by asking for it to the authors of 
~\cite{wendlinger_comparative_2024}. \

The preprocessing methodology encompasses for all the datasets the
removal of data points that are missing at least one feature.
Then, duplicate data points are removed to avoid overemphasizing
these elements and creating a bias in the data distribution. In
case the target labels are non-numeric, we modified them to
represent numerical classes. Finally, all the datasets except
the Iris Flower dataset are normalized using the standard
deviation and rescaled between a range of \(0\) and \(1\). This
is done with Scikit-learn~\cite{pedregosa_scikit-learn_2011}
and its purpose is to prevent feature dominance and to quicken
training convergence. \

There are two last details with regards to the preprocessing
of the datasets. Firstly, as mentioned in Subsection
~\ref{subsection:mnist}, the images from MNIST were downsampled
to be able to process them with a \ac{vqa}. Secondly, the majority
class from the \ac{pid} dataset was reduced to obtain a balanced
dataset with regards to the class distribution. \

\subsection{Configuration File}\label{subsection:config} \

The configuration files are essential to the training
pipeline. Its contents define the dataset to be used,
which type of \ac{qml} model is going to be trained,
which noise model will be injected, and some hyperparameters
regarding the \ac{qml} model's architecture. These hyperparameters
are the number of qubits, the number of layers, the number of
classes, the learning rate, the batch size, and the epochs. \

\begin{lstlisting}[language=Python, caption={Example configuration file.}, label=lst:config_file]
  {
    "dataset" : "diabetes",
    "qml_model" : "pqc",
    "noise_model" : "none",
    "num_qubits" : 3,
    "num_layers" : 40,
    "num_classes" : 2,
    "learning_rate" : 0.0005,
    "batch_size" : 16,
    "epochs" : 10
  }
\end{lstlisting}

An example configuration file can be seen in Listing
~\ref{lst:config_file}. This configuration file is given
to the training pipeline. Once the training is finished,
two more fields will be added to the configuration file.
These fields are the model accuracy on the test set and an
\(id\) to help identify the resulting model weights. The
configuration files are helpful to automate the training
and prepare the adversarial attacks on all the trained
\ac{qml} models. \

\subsection{Noise Injection}\label{subsection:noise_injection} \

The Circuit-Centric ansatz is built upon a combination of 
Pennylane's arbitrary rotation gate \colorbox{inline_gray}{\lstinline|Rot|}
and \colorbox{inline_gray}{\lstinline|CNOT|} gates. In the case
of Pennylane, the \(Rot\) gate is native to the framework, therefore,
there is no need to decompose it into other supported gates.
Nevertheless, if other frameworks or devices are utilized,
the gate decomposition is required and it would modify the resulting
trained model. \

\begin{equation}\label{eq:rot_decomposition}
  \Qcircuit @C=0.4em @R=0.4em {
    & \gate{Rot(\theta_{1},\theta_{2},\theta_{3})} & \gate{N(\epsilon)} & \qw \\
    & \gate{Rot(\theta_{1},\theta_{2},\theta_{3})} & \gate{N(\epsilon)} & \qw
  } \qquad
  \Qcircuit @C=0.4em @R=0.4em {
    & \gate{RZ(\theta_{1})} & \gate{N(\epsilon)} & \gate{RY(\theta_{2})} & \gate{N(\epsilon)} & \gate{RZ(\theta_{3})} & \gate{N(\epsilon)} & \qw \\
    & \gate{RZ(\theta_{1})} & \gate{N(\epsilon)} & \gate{RY(\theta_{2})} & \gate{N(\epsilon)} & \gate{RZ(\theta_{3})} & \gate{N(\epsilon)} & \qw
  }
\end{equation} \

Pennylane's \(Rot(\theta_{1},\theta_{2},\theta_{3})\) gate can be
decomposed in three rotational gates, namely the sequence of
\(R_{Z}(\theta_{1})\), \(R_{Y}(\theta_{2})\), and \(R_{Z}(\theta_{3})\).
If we injected the noise after every gate, the influence of noise on
the quantum circuit would increase almost three times (Eq.
~\ref{eq:rot_decomposition}). Consequently, we manually decompose the
\(Rot\) gate into its three corresponding gates to generate a single
layer in the Circuit-Centric ansatz. This will not only allow for the
pipeline in the future to be expanded and tested with new devices
and frameworks, but also will make the resuls of the thesis valid
for all of the devices and frameworks. \

In Equation~\ref{eq:rot_decomposition} the gate with the letter \(N\)
simulates the injected noise to the circuit. The type of noise model
to be injected to the \ac{qml} model is provided in the previously
described configuration file. The configuration file must then provide
one of the 6 possible noise models and either the corresponding probability
of noise to occur or the angle miscalibration \(\epsilon\). \

In order to inject noise to a \ac{qml} model we utilize Pennylane's
\colorbox{inline_gray}{\lstinline|transform|} module. We utilize it
in two different ways to create \ac{qml} models either with coherent
or incoherent noise. To simulate coherent noise we create a Pennylane
noise model object that will later on be applied to the quantum circuit.
The noise model is conformed by the addition of \(R_{Y}(\epsilon)\),
\(R_{Z}(\epsilon)\), or \(CRX(\epsilon)\) gates to the noiseless
\(R_{Y}(\theta)\), \(R_{Z}(\theta)\), or \(CNOT\) gates respectively
(where \(\epsilon\) represents the miscalibration angle). On the other
hand, to simulate incoherent noise we insert the noisy operation
depending on its type directly to the quantum circuit's gates.
This means that after every quantum gate noise might occur with
a given probability \(\epsilon\). \

For the incoherent noise \ac{qml} models, the chosen values for the
probability of noise occurring are \(2\%\), \(4\%\), \(6\%\), \(8\%\),
and \(10\%\). These values were chosen to be enough to notice an
influence in the training but not to overwhelm the classifier and
drown down any potential learning from the data. The same reasons
apply to the chosen values for the miscalibration angle for coherent
noise, the values are \(2^{\circ}\), \(4^{\circ}\), \(6^{\circ}\),
\(8^{\circ}\), and \(10^{\circ}\). \

\subsection{Training Pipeline}\label{subsection:pipeline} \

The stepping stones for the training pipeline were presented
in the previous subsections. The pipeline has been created
in order to simplify and automatize the training of the
\ac{qml} models. The pipeline is based on the configuration
files. Each \ac{qml} model requires a configuration file to
be trained, therefore, the amount of configuration files
depends on the number of models to be trained. \

In this thesis we are training \ac{qml} models for 7
datasets, with 6 noise types with each one having 5
different miscalibrations or probabilities. Adding up
the 7 noiseless models, we end up with 217 configuration
files and \ac{qml} models to be trained. Manually creating
the configuration files would take a considerable
amount of time. Therefore, we coded a script to
automatically create the configuration files of a
given dataset. \
% 7 * 6 * 5 = 210 + 7 (noiseless) = 217

After the configuration files have been created,
we can start with the training. Training 217
\ac{qml} models individually is also time consuming.
Thus, we created a script that retrieves all
the configuration files from a specific dataset
and performs the model training. More importantly,
the resulting weights of the model are automatically
saved in conjunction with the modified configuration
files. This serves two purposes. Firstly, we need the model
weights to evaluate their accuracy when performing the
adversarial attacks. Secondly, the pipeline checks
if the model has already been previously trained and
can skip retraining it. Therefore, the automated training
can be interrupted and resumed without losing any information
about the trained models. \

The actual model training is performed by using a training loop
based on the PyTorch Lightning~\cite{falcon_pytorch_2019} workflow.
To enable the Lightning workflow we created a helper file with
utility functions that help train and evaluate the \ac{qml} model.
In this file we define the training, validation and testing steps.
Additionally, we implement helper functions to calculate the 
accuracy and a threshold method to be able to evaluate the \ac{qml}
model. Finally, we define the model architecture based on the
weights, number of target labels to classify and a forward pass. \

The Lightning workflow simplifies and streamlines the training.
Nevertheless, in order to perform the training loop, a small
setup before is required. First of all, based on the configuration
file we retrieve the datasets. If the dataset has already been
fetched, we simply load the preprocessed dataset from the local files.
Later on, we define the \ac{qml} model based on the given hyperparameters.
Moreover, the required weights for training the module are randomly
initialized. \

Once the preprocessed dataset has been loaded, we divide it into
different sets to facilitate the training loop. For most of
the datasets we have a \(70/30\) split between the training
set and the test set. The test set is stratified according
to the data distribution from the training set. For the
MNIST and Plus-Minus datasets only \(20\%\) of the dataset
is used for training, as theses datasets are substantially
more computationally demanding than the other datasets. Moreover,
for the MNIST datasets we have a \(70/15/15\) training, 
validation, and test set split respectively. \

The last requirements for the training loop to be prepared are the
loss functionand the optimizer used to the adapt the \ac{qml} model. In our
specific case we utilize ADAM, a gradient-based optimizer, as it will
lead to faster convergence than just using a gradient descent. For the
loss function we utilize PyTorch's implementation of the Cross Entropy
Loss, therefore, we interpret the resulting measurements from the
\ac{qml} model as a probability distribution.  \

% easy to expand, incorporate new qml models and new noise values. \

\section{Adversarial Attacks on Variational Quantum Algorithm Model}\label{section:vqa_attacks} \

Mention cleverhans library and how it is used. Mention pipeline to create the adversarial samples. Mention the attack strenght ratio, take into account that the samples are being produced over preprocessed data.

\section{Experiments}\label{section:experiments} \

Possible experiments: \

a. Change the type of encoding \

b. Variational quantum circuits and kernels \

 1. Amplitude damping (open quantum systems / how the quantum system is affected by its environment) \

 2. Phase damping (loss of information but not energy) \

 3. Simpler phase and bit flips \