\chapter{Future Work}\label{chapter:future_work} \

In this chapter we discuss future possible research avenues related to
this thesis. First we suggest investigating the impact of weight initialization
on the trained \ac{qml} models. The weight initialization problem is not trivial
~\cite{mcclean_barren_2018} and it can highly affect the results of the
optimization. Another characteristic that can impact substantially the
trained \ac{qml} models is the optimizers used. In this thesis we optimized
the models using ADAM, a gradient based optimizer. Nevertheless, gradientless
based optimizing algorithms might improve \ac{qml} model training
~\cite{kulshrestha_learning_2023}. \ 

Furthermore, the effect of the chosen loss function when training \ac{qml}
models is still largely an open question. In this thesis we utilized
the Categorical Cross Entropy Loss, interpreting the \ac{qml} measurements
as a normalized probability distribution. This is a valid assumption
because the labels to classify are a power of two. However, when the classes
are not a power of two, this assumption is broken and choosing how to
attribute a measurement to a class becomes non-trivial. \

Moreover, utilizing different ans√§tze than \ac{vqa} would yield
different \ac{qml} models. We suggest exploring
a quantum kernel ansatz~\cite{hubregtsen_training_2022} to
verify the possible differences in model performance and resilience.
We also recommend trying different classical data encoding mechanisms
(e.g.\ angle encoding or data re-uploading~\cite{aminpour_strategic_2024})
as encoding might have a substantial influence in how features
are interpreted by the \ac{qml} models. \

Likewise, we would suggest using more complex datasets to visualize
any possible behavior change when training and attacking \ac{qml}
models. Currently, there are hardware limitations on quantum
simulations, thus, more complex datasets are not able to be trained.
When new more capable hardware becomes available, we would suggest
continuing this research with datasets like MNIST~\cite{bottou_comparison_1994}
or CIFAR-10~\cite{krizhevsky_learning_nodate}, as they are substantially more complex and delve
into the realm of computer vision. \

Additionally, focusing on adversarial attacks, other adversarial
techniques (e.g.~\cite{carlini_towards_2017}) could be utilized
to verify or dispute the trends derived in this thesis. Finally,
verifying the transferability of adversarial examples in between noisy
models, and also \ac{qml} models with different architectures, is of big importance
for training robust and resilient \ac{qml} classifiers. \
