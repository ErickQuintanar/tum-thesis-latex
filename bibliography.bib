
@book{national_academies_of_sciences_engineering_and_medicine_quantum_2019,
	title = {Quantum Computing: Progress and Prospects},
	isbn = {978-0-309-47972-1},
	shorttitle = {Quantum Computing},
	abstract = {Quantum mechanics, the subfield of physics that describes the behavior of very small (quantum) particles, provides the basis for a new paradigm of computing. First proposed in the 1980s as a way to improve computational modeling of quantum systems, the field of quantum computing has recently garnered significant attention due to progress in building small-scale devices. However, significant technical advances will be required before a large-scale, practical quantum computer can be achieved. Quantum Computing: Progress and Prospects provides an introduction to the field, including the unique characteristics and constraints of the technology, and assesses the feasibility and implications of creating a functional quantum computer capable of addressing real-world problems. This report considers hardware and software requirements, quantum algorithms, drivers of advances in quantum computing and quantum devices, benchmarks associated with relevant use cases, the time and resources required, and how to assess the probability of success.},
	pagetotal = {273},
	publisher = {National Academies Press},
	author = {National Academies of Sciences, Engineering, \{and\} Medicine},
	date = {2019-03-27},
	langid = {english},
	note = {Google-Books-{ID}: {ATH}3DwAAQBAJ},
	keywords = {Computers / General, Computers / Information Technology},
}

@incollection{shor_quantum_nodate,
	title = {Quantum Computing},
	pages = {467--486},
	booktitle = {Documenta Mathematica - Extra Volume {ICM} 1998},
	author = {Shor, Peter W.},
}

@misc{bommasani_opportunities_2022,
	title = {On the Opportunities and Risks of Foundation Models},
	url = {http://arxiv.org/abs/2108.07258},
	abstract = {{AI} is undergoing a paradigm shift with the rise of models (e.g., {BERT}, {DALL}-E, {GPT}-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.},
	number = {{arXiv}:2108.07258},
	publisher = {{arXiv}},
	author = {Bommasani, Rishi and Hudson, Drew A. and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S. and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and Brynjolfsson, Erik and Buch, Shyamal and Card, Dallas and Castellon, Rodrigo and Chatterji, Niladri and Chen, Annie and Creel, Kathleen and Davis, Jared Quincy and Demszky, Dora and Donahue, Chris and Doumbouya, Moussa and Durmus, Esin and Ermon, Stefano and Etchemendy, John and Ethayarajh, Kawin and Fei-Fei, Li and Finn, Chelsea and Gale, Trevor and Gillespie, Lauren and Goel, Karan and Goodman, Noah and Grossman, Shelby and Guha, Neel and Hashimoto, Tatsunori and Henderson, Peter and Hewitt, John and Ho, Daniel E. and Hong, Jenny and Hsu, Kyle and Huang, Jing and Icard, Thomas and Jain, Saahil and Jurafsky, Dan and Kalluri, Pratyusha and Karamcheti, Siddharth and Keeling, Geoff and Khani, Fereshte and Khattab, Omar and Koh, Pang Wei and Krass, Mark and Krishna, Ranjay and Kuditipudi, Rohith and Kumar, Ananya and Ladhak, Faisal and Lee, Mina and Lee, Tony and Leskovec, Jure and Levent, Isabelle and Li, Xiang Lisa and Li, Xuechen and Ma, Tengyu and Malik, Ali and Manning, Christopher D. and Mirchandani, Suvir and Mitchell, Eric and Munyikwa, Zanele and Nair, Suraj and Narayan, Avanika and Narayanan, Deepak and Newman, Ben and Nie, Allen and Niebles, Juan Carlos and Nilforoshan, Hamed and Nyarko, Julian and Ogut, Giray and Orr, Laurel and Papadimitriou, Isabel and Park, Joon Sung and Piech, Chris and Portelance, Eva and Potts, Christopher and Raghunathan, Aditi and Reich, Rob and Ren, Hongyu and Rong, Frieda and Roohani, Yusuf and Ruiz, Camilo and Ryan, Jack and Ré, Christopher and Sadigh, Dorsa and Sagawa, Shiori and Santhanam, Keshav and Shih, Andy and Srinivasan, Krishnan and Tamkin, Alex and Taori, Rohan and Thomas, Armin W. and Tramèr, Florian and Wang, Rose E. and Wang, William and Wu, Bohan and Wu, Jiajun and Wu, Yuhuai and Xie, Sang Michael and Yasunaga, Michihiro and You, Jiaxuan and Zaharia, Matei and Zhang, Michael and Zhang, Tianyi and Zhang, Xikun and Zhang, Yuhui and Zheng, Lucia and Zhou, Kaitlyn and Liang, Percy},
	urldate = {2024-04-03},
	date = {2022-07-12},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2108.07258 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society, Computer Science - Machine Learning},
}

@misc{madry_towards_2019,
	title = {Towards Deep Learning Models Resistant to Adversarial Attacks},
	url = {http://arxiv.org/abs/1706.06083},
	abstract = {Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at https://github.com/{MadryLab}/mnist\_challenge and https://github.com/{MadryLab}/cifar10\_challenge.},
	number = {{arXiv}:1706.06083},
	publisher = {{arXiv}},
	author = {Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
	urldate = {2024-04-03},
	date = {2019-09-04},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1706.06083 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@inproceedings{carlini_towards_2017,
	location = {San Jose, {CA}, {USA}},
	title = {Towards Evaluating the Robustness of Neural Networks},
	isbn = {978-1-5090-5533-3},
	url = {http://ieeexplore.ieee.org/document/7958570/},
	doi = {10.1109/SP.2017.49},
	abstract = {Neural networks provide state-of-the-art results for most machine learning tasks. Unfortunately, neural networks are vulnerable to adversarial examples: given an input x and any target classiﬁcation t, it is possible to ﬁnd a new input x that is similar to x but classiﬁed as t. This makes it difﬁcult to apply neural networks in security-critical areas. Defensive distillation is a recently proposed approach that can take an arbitrary neural network, and increase its robustness, reducing the success rate of current attacks’ ability to ﬁnd adversarial examples from 95\% to 0.5\%.},
	eventtitle = {2017 {IEEE} Symposium on Security and Privacy ({SP})},
	pages = {39--57},
	booktitle = {2017 {IEEE} Symposium on Security and Privacy ({SP})},
	publisher = {{IEEE}},
	author = {Carlini, Nicholas and Wagner, David},
	urldate = {2024-04-03},
	date = {2017-05},
	langid = {english},
}

@misc{papernot_transferability_2016,
	title = {Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples},
	url = {http://arxiv.org/abs/1605.07277},
	shorttitle = {Transferability in Machine Learning},
	abstract = {Many machine learning models are vulnerable to adversarial examples: inputs that are specially crafted to cause a machine learning model to produce an incorrect output. Adversarial examples that aﬀect one model often aﬀect another model, even if the two models have diﬀerent architectures or were trained on diﬀerent training sets, so long as both models were trained to perform the same task. An attacker may therefore train their own substitute model, craft adversarial examples against the substitute, and transfer them to a victim model, with very little information about the victim. Recent work has further developed a technique that uses the victim model as an oracle to label a synthetic training set for the substitute, so the attacker need not even collect a training set to mount the attack. We extend these recent techniques using reservoir sampling to greatly enhance the eﬃciency of the training procedure for the substitute model. We introduce new transferability attacks between previously unexplored (substitute, victim) pairs of machine learning model classes, most notably {SVMs} and decision trees. We demonstrate our attacks on two commercial machine learning classiﬁcation systems from Amazon (96.19\% misclassiﬁcation rate) and Google (88.94\%) using only 800 queries of the victim model, thereby showing that existing machine learning approaches are in general vulnerable to systematic black-box attacks regardless of their structure.},
	number = {{arXiv}:1605.07277},
	publisher = {{arXiv}},
	author = {Papernot, Nicolas and {McDaniel}, Patrick and Goodfellow, Ian},
	urldate = {2024-04-03},
	date = {2016-05-23},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1605.07277 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Cryptography and Security},
}

@misc{kurakin_adversarial_2017,
	title = {Adversarial Machine Learning at Scale},
	url = {http://arxiv.org/abs/1611.01236},
	abstract = {Adversarial examples are malicious inputs designed to fool machine learning models. They often transfer from one model to another, allowing attackers to mount black box attacks without knowledge of the target model’s parameters. Adversarial training is the process of explicitly training a model on adversarial examples, in order to make it more robust to attack or to reduce its test error on clean inputs. So far, adversarial training has primarily been applied to small problems. In this research, we apply adversarial training to {ImageNet} (Russakovsky et al., 2014). Our contributions include: (1) recommendations for how to succesfully scale adversarial training to large models and datasets, (2) the observation that adversarial training confers robustness to single-step attack methods, (3) the ﬁnding that multi-step attack methods are somewhat less transferable than singlestep attack methods, so single-step attacks are the best for mounting black-box attacks, and (4) resolution of a “label leaking” effect that causes adversarially trained models to perform better on adversarial examples than on clean examples, because the adversarial example construction process uses the true label and the model can learn to exploit regularities in the construction process.},
	number = {{arXiv}:1611.01236},
	publisher = {{arXiv}},
	author = {Kurakin, Alexey and Goodfellow, Ian and Bengio, Samy},
	urldate = {2024-04-03},
	date = {2017-02-10},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1611.01236 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Cryptography and Security, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{goodfellow_explaining_2015,
	title = {Explaining and Harnessing Adversarial Examples},
	url = {http://arxiv.org/abs/1412.6572},
	abstract = {Several machine learning models, including neural networks, consistently misclassify adversarial examples—inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high conﬁdence. Early attempts at explaining this phenomenon focused on nonlinearity and overﬁtting. We argue instead that the primary cause of neural networks’ vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the ﬁrst explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the {MNIST} dataset.},
	number = {{arXiv}:1412.6572},
	publisher = {{arXiv}},
	author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
	urldate = {2024-04-03},
	date = {2015-03-20},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1412.6572 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{szegedy_intriguing_2014,
	title = {Intriguing properties of neural networks},
	url = {http://arxiv.org/abs/1312.6199},
	abstract = {Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties.},
	number = {{arXiv}:1312.6199},
	publisher = {{arXiv}},
	author = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
	urldate = {2024-04-03},
	date = {2014-02-19},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1312.6199 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Computer Vision and Pattern Recognition},
}

@article{preskill_quantum_2018,
	title = {Quantum Computing in the {NISQ} era and beyond},
	volume = {2},
	issn = {2521-327X},
	url = {http://arxiv.org/abs/1801.00862},
	doi = {10.22331/q-2018-08-06-79},
	abstract = {Noisy Intermediate-Scale Quantum ({NISQ}) technology will be available in the near future. Quantum computers with 50-100 qubits may be able to perform tasks which surpass the capabilities of today's classical digital computers, but noise in quantum gates will limit the size of quantum circuits that can be executed reliably. {NISQ} devices will be useful tools for exploring many-body quantum physics, and may have other useful applications, but the 100-qubit quantum computer will not change the world right away --- we should regard it as a significant step toward the more powerful quantum technologies of the future. Quantum technologists should continue to strive for more accurate quantum gates and, eventually, fully fault-tolerant quantum computing.},
	pages = {79},
	journaltitle = {Quantum},
	shortjournal = {Quantum},
	author = {Preskill, John},
	urldate = {2024-04-03},
	date = {2018-08-06},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1801.00862 [cond-mat, physics:quant-ph]},
	keywords = {Condensed Matter - Strongly Correlated Electrons, Quantum Physics},
}

@article{ciliberto_quantum_2018,
	title = {Quantum machine learning: a classical perspective},
	volume = {474},
	issn = {1364-5021, 1471-2946},
	url = {https://royalsocietypublishing.org/doi/10.1098/rspa.2017.0551},
	doi = {10.1098/rspa.2017.0551},
	shorttitle = {Quantum machine learning},
	abstract = {Recently, increased computational power and data availability, as well as algorithmic advances, have led machine learning ({ML}) techniques to impressive results in regression, classification, data generation and reinforcement learning tasks. Despite these successes, the proximity to the physical limits of chip fabrication alongside the increasing size of datasets is motivating a growing number of researchers to explore the possibility of harnessing the power of quantum computation to speed up classical {ML} algorithms. Here we review the literature in quantum {ML} and discuss perspectives for a mixed readership of classical {ML} and quantum computation experts. Particular emphasis will be placed on clarifying the limitations of quantum algorithms, how they compare with their best classical counterparts and why quantum resources are expected to provide advantages for learning problems. Learning in the presence of noise and certain computationally hard problems in {ML} are identified as promising directions for the field. Practical questions, such as how to upload classical data into quantum form, will also be addressed.},
	pages = {20170551},
	number = {2209},
	journaltitle = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	shortjournal = {Proc. R. Soc. A.},
	author = {Ciliberto, Carlo and Herbster, Mark and Ialongo, Alessandro Davide and Pontil, Massimiliano and Rocchetto, Andrea and Severini, Simone and Wossnig, Leonard},
	urldate = {2024-03-28},
	date = {2018-01},
	langid = {english},
}

@article{shor_polynomial-time_1997,
	title = {Polynomial-Time Algorithms for Prime Factorization and Discrete Logarithms on a Quantum Computer},
	volume = {26},
	issn = {0097-5397},
	url = {https://epubs.siam.org/doi/10.1137/S0097539795293172},
	doi = {10.1137/S0097539795293172},
	abstract = {Recently a great deal of attention has been focused on quantum computation following a sequence of results [Bernstein and Vazirani, in Proc. 25th Annual {ACM} Symposium Theory Comput., 1993, pp. 11--20, {SIAM} J. Comput., 26 (1997), pp. 1277--1339], [Simon, in Proc. 35th Annual {IEEE} Symposium Foundations Comput. Sci., 1994, pp. 116--123, {SIAM} J. Comput., 26 (1997), pp. 1340--1349], [Shor, in Proc. 35th Annual {IEEE} Symposium Foundations Comput. Sci., 1994, pp. 124--134] suggesting that quantum computers are more powerful than classical probabilistic computers. Following Shor's result that factoring and the extraction of discrete logarithms are both solvable in quantum polynomial time, it is natural to ask whether all of \${\textbackslash}{NP}\$ can be efficiently solved in quantum polynomial time. In this paper, we address this question by proving that relative to an oracle chosen uniformly at random with probability 1 the class \${\textbackslash}{NP}\$ cannot be solved on a quantum Turing machine ({QTM}) in time \$o(2{\textasciicircum}\{n/2\})\$. We also show that relative to a permutation oracle chosen uniformly at random with probability 1 the class \${\textbackslash}{NP} {\textbackslash}cap {\textbackslash}{coNP}\$ cannot be solved on a {QTM} in time \$o(2{\textasciicircum}\{n/3\})\$. The former bound is tight since recent work of Grover [in \{{\textbackslash}it Proc.{\textbackslash} \$28\$th Annual {ACM} Symposium Theory Comput.\}, 1996] shows how to accept the class \${\textbackslash}{NP}\$ relative to any oracle on a quantum computer in time \$O(2{\textasciicircum}\{n/2\})\$.},
	pages = {1484--1509},
	number = {5},
	journaltitle = {{SIAM} Journal on Computing},
	shortjournal = {{SIAM} J. Comput.},
	author = {Shor, Peter W.},
	urldate = {2024-03-28},
	date = {1997-10},
	note = {Publisher: Society for Industrial and Applied Mathematics},
}

@article{van_dam_quantum_2006,
	title = {Quantum Algorithms for Some Hidden Shift Problems},
	volume = {36},
	issn = {0097-5397, 1095-7111},
	url = {http://epubs.siam.org/doi/10.1137/S009753970343141X},
	doi = {10.1137/S009753970343141X},
	abstract = {Almost all of the most successful quantum algorithms discovered to date exploit the ability of the Fourier transform to recover subgroup structures of functions, especially periodicity. The fact that Fourier transforms can also be used to capture shift structure has received far less attention in the context of quantum computation. In this paper, we present three examples of “unknown shift” problems that can be solved eﬃciently on a quantum computer using the quantum Fourier transform. For one of these problems, the shifted Legendre symbol problem, we give evidence that the problem is hard to solve classically, by showing a reduction from breaking algebraically homomorphic cryptosystems. We also deﬁne the hidden coset problem, which generalizes the hidden shift problem and the hidden subgroup problem. This framework provides a uniﬁed way of viewing the ability of the Fourier transform to capture subgroup and shift structure.},
	pages = {763--778},
	number = {3},
	journaltitle = {{SIAM} Journal on Computing},
	shortjournal = {{SIAM} J. Comput.},
	author = {Van Dam, Wim and Hallgren, Sean and Ip, Lawrence},
	urldate = {2024-03-28},
	date = {2006-01},
	langid = {english},
}

@article{hallgren_polynomial-time_2007,
	title = {Polynomial-time quantum algorithms for Pell's equation and the principal ideal problem},
	volume = {54},
	issn = {0004-5411, 1557-735X},
	url = {https://dl.acm.org/doi/10.1145/1206035.1206039},
	doi = {10.1145/1206035.1206039},
	abstract = {We give polynomial-time quantum algorithms for three problems from computational algebraic number theory. The ﬁrst is Pell’s equation. Given a positive nonsquare integer d, Pell’s equation is x2 − dy2 = 1 and the goal is to ﬁnd its integer solutions. Factoring integers reduces to ﬁnding integer solutions of Pell’s equation, but a reduction in the other direction is not known and appears more difﬁcult. The second problem we solve is the principal ideal problem in real quadratic number ﬁelds. This problem, which is at least as hard as solving Pell’s equation, is the one-way function underlying the Buchmann–Williams key exchange system, which is therefore broken by our quantum algorithm. Finally, assuming the generalized Riemann hypothesis, this algorithm can be used to compute the class group of a real quadratic number ﬁeld.},
	pages = {1--19},
	number = {1},
	journaltitle = {Journal of the {ACM}},
	shortjournal = {J. {ACM}},
	author = {Hallgren, Sean},
	urldate = {2024-03-28},
	date = {2007-03},
	langid = {english},
}

@online{nielsen_quantum_2010,
	title = {Quantum Computation and Quantum Information: 10th Anniversary Edition},
	url = {https://www.cambridge.org/highereducation/books/quantum-computation-and-quantum-information/01E10196D0A682A6AEFFEA52D53BE9AE},
	shorttitle = {Quantum Computation and Quantum Information},
	abstract = {One of the most cited books in physics of all time, Quantum Computation and Quantum Information remains the best textbook in this exciting field of science. This 10th anniversary edition includes an introduction from the authors setting the work in context. This comprehensive textbook describes such remarkable effects as fast quantum algorithms, quantum teleportation, quantum cryptography and quantum error-correction. Quantum mechanics and computer science are introduced before moving on to describe what a quantum computer is, how it can be used to solve problems faster than 'classical' computers and its real-world implementation. It concludes with an in-depth treatment of quantum information. Containing a wealth of figures and exercises, this well-known textbook is ideal for courses on the subject, and will interest beginning graduate students and researchers in physics, computer science, mathematics, and electrical engineering.},
	titleaddon = {Higher Education from Cambridge University Press},
	author = {Nielsen, Michael A. and Chuang, Isaac L.},
	urldate = {2024-03-28},
	date = {2010-12-09},
	langid = {english},
	doi = {10.1017/CBO9780511976667},
	note = {{ISBN}: 9780511976667
Publisher: Cambridge University Press},
	file = {Snapshot:C\:\\Users\\erick\\Zotero\\storage\\UKZ5DK7N\\01E10196D0A682A6AEFFEA52D53BE9AE.html:text/html},
}

@book{schuld_machine_2021,
	location = {Cham},
	title = {Machine Learning with Quantum Computers},
	rights = {https://www.springer.com/tdm},
	isbn = {978-3-030-83097-7 978-3-030-83098-4},
	url = {https://link.springer.com/10.1007/978-3-030-83098-4},
	series = {Quantum Science and Technology},
	publisher = {Springer International Publishing},
	author = {Schuld, Maria and Petruccione, Francesco},
	urldate = {2024-03-28},
	date = {2021},
	langid = {english},
	doi = {10.1007/978-3-030-83098-4},
}

@article{rivest_method_1978,
	title = {A method for obtaining digital signatures and public-key cryptosystems},
	volume = {21},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/359340.359342},
	doi = {10.1145/359340.359342},
	abstract = {An encryption method is presented with the novel property that publicly revealing an encryption key does not thereby reveal the corresponding decryption key. This has two important consequences: (1) Couriers or other secure means are not needed to transmit keys, since a message can be enciphered using an encryption key publicly revealed by the intented recipient. Only he can decipher the message, since only he knows the corresponding decryption key. (2) A message can be “signed” using a privately held decryption key. Anyone can verify this signature using the corresponding publicly revealed encryption key. Signatures cannot be forged, and a signer cannot later deny the validity of his signature. This has obvious applications in “electronic mail” and “electronic funds transfer” systems. A message is encrypted by representing it as a number M, raising M to a publicly specified power e, and then taking the remainder when the result is divided by the publicly specified product,
              n
              , of two large secret primer numbers p and q. Decryption is similar; only a different, secret, power d is used, where e * d ≡ 1(mod (p - 1) * (q - 1)). The security of the system rests in part on the difficulty of factoring the published divisor,
              n
              .},
	pages = {120--126},
	number = {2},
	journaltitle = {Communications of the {ACM}},
	shortjournal = {Commun. {ACM}},
	author = {Rivest, R. L. and Shamir, A. and Adleman, L.},
	urldate = {2024-04-04},
	date = {1978-02},
	langid = {english},
}

@article{bloch_nuclear_1946,
	title = {Nuclear Induction},
	volume = {70},
	rights = {http://link.aps.org/licenses/aps-default-license},
	issn = {0031-899X},
	url = {https://link.aps.org/doi/10.1103/PhysRev.70.460},
	doi = {10.1103/PhysRev.70.460},
	pages = {460--474},
	number = {7},
	journaltitle = {Physical Review},
	shortjournal = {Phys. Rev.},
	author = {Bloch, F.},
	urldate = {2024-04-08},
	date = {1946-10-01},
	langid = {english},
}

@article{born_quantenmechanik_1926,
	title = {Quantenmechanik der Stoßvorgänge},
	volume = {38},
	issn = {0044-3328},
	url = {https://doi.org/10.1007/BF01397184},
	doi = {10.1007/BF01397184},
	abstract = {Die Schrödingersche Form der Quantenmechanik erlaubt in natürlicher Weise die Häufigkeit eines Zustandes zu definieren mit Hilfe der Intensität der zugeordneten Eigenschwingung. Diese Auffassung führt zu einer Theorie der Stoß-vorgänge, bei der die Übergangswahrscheinlichkeiten durch das asymptotische Verhalten aperiodischer Lösungen bestimmt werden.},
	pages = {803--827},
	number = {11},
	journaltitle = {Zeitschrift für Physik},
	shortjournal = {Z. Physik},
	author = {Born, Max},
	urldate = {2024-04-08},
	date = {1926-11-01},
	langid = {german},
}

@article{dirac_new_1939,
	title = {A New Notation for Quantum Mechanics},
	volume = {35},
	issn = {1469-8064, 0305-0041},
	url = {https://www.cambridge.org/core/journals/mathematical-proceedings-of-the-cambridge-philosophical-society/article/new-notation-for-quantum-mechanics/4631DB9213D680D6332BA11799D76AFB},
	doi = {10.1017/S0305004100021162},
	abstract = {In mathematical theories the question of notation, while not of primary importance, is yet worthy of careful consideration, since a good notation can be of great value in helping the development of a theory, by making it easy to write down those quantities or combinations of quantities that are important, and difficult or impossible to write down those that are unimportant. The summation convention in tensor analysis is an example, illustrating how specially appropriate a notation can be.},
	pages = {416--418},
	number = {3},
	journaltitle = {Mathematical Proceedings of the Cambridge Philosophical Society},
	author = {Dirac, P. a. M.},
	urldate = {2024-04-08},
	date = {1939-07},
	langid = {english},
}

@article{schumacher_quantum_1995,
	title = {Quantum Coding},
	volume = {51},
	rights = {http://link.aps.org/licenses/aps-default-license},
	issn = {1050-2947, 1094-1622},
	url = {https://link.aps.org/doi/10.1103/PhysRevA.51.2738},
	doi = {10.1103/PhysRevA.51.2738},
	pages = {2738--2747},
	number = {4},
	journaltitle = {Physical Review A},
	shortjournal = {Phys. Rev. A},
	author = {Schumacher, Benjamin},
	urldate = {2024-04-05},
	date = {1995-04-01},
	langid = {english},
}

@article{harris_array_2020,
	title = {Array programming with {NumPy}},
	volume = {585},
	rights = {2020 The Author(s)},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-020-2649-2},
	doi = {10.1038/s41586-020-2649-2},
	abstract = {Array programming provides a powerful, compact and expressive syntax for accessing, manipulating and operating on data in vectors, matrices and higher-dimensional arrays. {NumPy} is the primary array programming library for the Python language. It has an essential role in research analysis pipelines in fields as diverse as physics, chemistry, astronomy, geoscience, biology, psychology, materials science, engineering, finance and economics. For example, in astronomy, {NumPy} was an important part of the software stack used in the discovery of gravitational waves1 and in the first imaging of a black hole2. Here we review how a few fundamental array concepts lead to a simple and powerful programming paradigm for organizing, exploring and analysing scientific data. {NumPy} is the foundation upon which the scientific Python ecosystem is constructed. It is so pervasive that several projects, targeting audiences with specialized needs, have developed their own {NumPy}-like interfaces and array objects. Owing to its central position in the ecosystem, {NumPy} increasingly acts as an interoperability layer between such array computation libraries and, together with its application programming interface ({API}), provides a flexible framework to support the next decade of scientific and industrial analysis.},
	pages = {357--362},
	number = {7825},
	journaltitle = {Nature},
	author = {Harris, Charles R. and Millman, K. Jarrod and van der Walt, Stéfan J. and Gommers, Ralf and Virtanen, Pauli and Cournapeau, David and Wieser, Eric and Taylor, Julian and Berg, Sebastian and Smith, Nathaniel J. and Kern, Robert and Picus, Matti and Hoyer, Stephan and van Kerkwijk, Marten H. and Brett, Matthew and Haldane, Allan and del Río, Jaime Fernández and Wiebe, Mark and Peterson, Pearu and Gérard-Marchant, Pierre and Sheppard, Kevin and Reddy, Tyler and Weckesser, Warren and Abbasi, Hameer and Gohlke, Christoph and Oliphant, Travis E.},
	urldate = {2024-04-10},
	date = {2020-09},
	langid = {english},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computational neuroscience, Computational science, Computer science, Software, Solar physics},
}

@article{pravia_robust_2003,
	title = {Robust Control of Quantum Information},
	volume = {119},
	issn = {0021-9606},
	url = {https://doi.org/10.1063/1.1619132},
	doi = {10.1063/1.1619132},
	abstract = {Errors in the control of quantum systems may be classified as unitary, decoherent, and incoherent. Unitary errors are systematic, and result in a density matrix that differs from the desired one by a unitary operation. Decoherent errors correspond to general completely positive superoperators, and can only be corrected using methods such as quantum error correction. Incoherent errors can also be described, on average, by completely positive superoperators, but can nevertheless be corrected by the application of a locally unitary operation that “refocuses” them. They are due to reproducible spatial or temporal variations in the system’s Hamiltonian, so that information on the variations is encoded in the system’s spatiotemporal state and can be used to correct them. In this paper liquid-state nuclear magnetic resonance is used to demonstrate that such refocusing effects can be built directly into the control fields, where the incoherence arises from spatial inhomogeneities in the quantizing static magnetic field as well as the radio-frequency control fields themselves. Using perturbation theory, it is further shown that the eigenvalue spectrum of the completely positive superoperator exhibits a characteristic spread that contains information on the Hamiltonians’ underlying distribution.},
	pages = {9993--10001},
	number = {19},
	journaltitle = {The Journal of Chemical Physics},
	shortjournal = {The Journal of Chemical Physics},
	author = {Pravia, Marco A. and Boulant, Nicolas and Emerson, Joseph and Farid, Amro and Fortunato, Evan M. and Havel, Timothy F. and Martinez, R. and Cory, David G.},
	urldate = {2024-05-02},
	date = {2003-11-15},
}

@article{yoneda_quantum-dot_2018,
	title = {A Quantum-Dot Spin Qubit with Coherence Limited by Charge Noise and Fidelity Higher than 99.9\%},
	volume = {13},
	rights = {2017 The Author(s)},
	issn = {1748-3395},
	url = {https://www.nature.com/articles/s41565-017-0014-x},
	doi = {10.1038/s41565-017-0014-x},
	abstract = {The isolation of qubits from noise sources, such as surrounding nuclear spins and spin–electric susceptibility1–4, has enabled extensions of quantum coherence times in recent pivotal advances towards the concrete implementation of spin-based quantum computation. In fact, the possibility of achieving enhanced quantum coherence has been substantially doubted for nanostructures due to the characteristic high degree of background charge fluctuations5–7. Still, a sizeable spin–electric coupling will be needed in realistic multiple-qubit systems to address single-spin and spin–spin manipulations8–10. Here, we realize a single-electron spin qubit with an isotopically enriched phase coherence time (20 μs)11,12and fast electrical control speed (up to 30 {MHz}) mediated by extrinsic spin–electric coupling. Using rapid spin rotations, we reveal that the free-evolution dephasing is caused by charge noise—rather than conventional magnetic noise—as highlighted by a 1/f spectrum extended over seven decades of frequency. The qubit exhibits superior performance with single-qubit gate fidelities exceeding 99.9\% on average, offering a promising route to large-scale spin-qubit systems with fault-tolerant controllability.},
	pages = {102--106},
	number = {2},
	journaltitle = {Nature Nanotechnology},
	shortjournal = {Nature Nanotech},
	author = {Yoneda, Jun and Takeda, Kenta and Otsuka, Tomohiro and Nakajima, Takashi and Delbecq, Matthieu R. and Allison, Giles and Honda, Takumu and Kodera, Tetsuo and Oda, Shunri and Hoshi, Yusuke and Usami, Noritaka and Itoh, Kohei M. and Tarucha, Seigo},
	urldate = {2024-05-02},
	date = {2018-02},
	langid = {english},
	note = {Publisher: Nature Publishing Group},
	keywords = {Quantum dots, Qubits},
}

@article{einstein_can_1935,
	title = {Can Quantum-Mechanical Description of Physical Reality Be Considered Complete?},
	volume = {47},
	rights = {https://link.aps.org/licenses/aps-default-license},
	issn = {0031-899X},
	url = {https://link.aps.org/doi/10.1103/PhysRev.47.777},
	doi = {10.1103/PhysRev.47.777},
	pages = {777--780},
	number = {10},
	journaltitle = {Physical Review},
	shortjournal = {Phys. Rev.},
	author = {Einstein, A. and Podolsky, B. and Rosen, N.},
	urldate = {2024-04-16},
	date = {1935-05-15},
	langid = {english},
}

@article{bennett_teleporting_1993,
	title = {Teleporting an Unknown Quantum State via Dual Classical and Einstein-Podolsky-Rosen Channels},
	volume = {70},
	rights = {http://link.aps.org/licenses/aps-default-license},
	issn = {0031-9007},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.70.1895},
	doi = {10.1103/PhysRevLett.70.1895},
	pages = {1895--1899},
	number = {13},
	journaltitle = {Physical Review Letters},
	shortjournal = {Phys. Rev. Lett.},
	author = {Bennett, Charles H. and Brassard, Gilles and Crépeau, Claude and Jozsa, Richard and Peres, Asher and Wootters, William K.},
	urldate = {2024-04-15},
	date = {1993-03-29},
	langid = {english},
}

@article{bennett_communication_1992,
	title = {Communication via One- and Two-Particle Operators on Einstein-Podolsky-Rosen States},
	volume = {69},
	rights = {http://link.aps.org/licenses/aps-default-license},
	issn = {0031-9007},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.69.2881},
	doi = {10.1103/PhysRevLett.69.2881},
	pages = {2881--2884},
	number = {20},
	journaltitle = {Physical Review Letters},
	shortjournal = {Phys. Rev. Lett.},
	author = {Bennett, Charles H. and Wiesner, Stephen J.},
	urldate = {2024-04-15},
	date = {1992-11-16},
	langid = {english},
}

@article{resch_benchmarking_2022,
	title = {Benchmarking Quantum Computers and the Impact of Quantum Noise},
	volume = {54},
	issn = {0360-0300, 1557-7341},
	url = {https://dl.acm.org/doi/10.1145/3464420},
	doi = {10.1145/3464420},
	abstract = {Benchmarking is how the performance of a computing system is determined. Surprisingly, even for classical computers this is not a straightforward process. One must choose the appropriate benchmark and metrics to extract meaningful results. Different benchmarks test the system in different ways, and each individual metric may or may not be of interest. Choosing the appropriate approach is tricky. The situation is even more open ended for quantum computers, where there is a wider range of hardware, fewer established guidelines, and additional complicating factors. Notably, quantum noise significantly impacts performance and is difficult to model accurately. Here, we discuss benchmarking of quantum computers from a computer architecture perspective and provide numerical simulations highlighting challenges that suggest caution.},
	pages = {1--35},
	number = {7},
	journaltitle = {{ACM} Computing Surveys},
	shortjournal = {{ACM} Comput. Surv.},
	author = {Resch, Salonik and Karpuzcu, Ulya R.},
	urldate = {2024-05-03},
	date = {2022-09-30},
	langid = {english},
}

@article{iverson_coherence_2020,
	title = {Coherence in Logical Quantum Channels},
	volume = {22},
	issn = {1367-2630},
	url = {https://iopscience.iop.org/article/10.1088/1367-2630/ab8e5c},
	doi = {10.1088/1367-2630/ab8e5c},
	abstract = {We study the effectiveness of quantum error correction against coherent noise. Coherent errors (for example, unitary noise) can interfere constructively, so that in some cases the average inﬁdelity of a quantum circuit subjected to coherent errors may increase quadratically with the circuit size; in contrast, when errors are incoherent (for example, depolarizing noise), the average inﬁdelity increases at worst linearly with circuit size. We consider the performance of quantum stabilizer codes against a noise model in which a unitary rotation is applied to each qubit, where the axes and angles of rotation are nearly the same for all qubits. In particular, we show that for the toric code subject to such independent coherent noise, and for minimal-weight decoding, the logical channel after error correction becomes increasingly incoherent as the length of the code increases, provided the noise strength decays inversely with the code distance. A similar conclusion holds for weakly correlated coherent noise. Our methods can also be used for analyzing the performance of other codes and fault-tolerant protocols against coherent noise. However, our result does not show that the coherence of the logical channel is suppressed in the more physically relevant case where the noise strength is held constant as the code block grows, and we recount the difﬁculties that prevented us from extending the result to that case. Nevertheless our work supports the idea that fault-tolerant quantum computing schemes will work effectively against coherent noise, providing encouraging news for quantum hardware builders who worry about the damaging effects of control errors and coherent interactions with the environment.},
	pages = {073066},
	number = {7},
	journaltitle = {New Journal of Physics},
	shortjournal = {New J. Phys.},
	author = {Iverson, Joseph K and Preskill, John},
	urldate = {2024-05-03},
	date = {2020-07-01},
	langid = {english},
}

@article{boulant_incoherent_2004,
	title = {Incoherent Noise and Quantum Information Processing},
	volume = {121},
	issn = {0021-9606, 1089-7690},
	url = {http://arxiv.org/abs/quant-ph/0312116},
	doi = {10.1063/1.1773161},
	abstract = {Incoherence in the controlled Hamiltonian is an important limitation on the precision of coherent control in quantum information processing. Incoherence can typically be modelled as a distribution of unitary processes arising from slowly varying experimental parameters. We show how it introduces artifacts in quantum process tomography and we explain how the resulting estimate of the superoperator may not be completely positive. We then go on to attack the inverse problem of extracting an eﬀective distribution of unitaries that characterizes the incoherence via a perturbation theory analysis of the superoperator eigenvalue spectra.},
	pages = {2955--2961},
	number = {7},
	journaltitle = {The Journal of Chemical Physics},
	author = {Boulant, N. and Furuta, S. and Emerson, J. and Havel, T. F. and Cory, D. G.},
	urldate = {2024-05-03},
	date = {2004-08-15},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {quant-ph/0312116},
	keywords = {Quantum Physics},
}

@misc{kaufmann_characterization_2023,
	title = {Characterization of Coherent Errors in Noisy Quantum Devices},
	url = {http://arxiv.org/abs/2307.08741},
	abstract = {Characterization of quantum devices generates insights into their sources of disturbances. State-of-the-art characterization protocols often focus on incoherent noise and eliminate coherent errors when using Pauli or Clifford twirling techniques. This approach biases the structure of the effective noise and adds a circuit and sampling overhead. We motivate the extension of an incoherent local Pauli noise model to coherent errors and present a practical characterization protocol for an arbitrary gate layer. We demonstrate our protocol on a superconducting hardware platform and identify the leading coherent errors. To verify the characterized noise structure, we mitigate its coherent and incoherent components using a gate-level coherent noise mitigation scheme in conjunction with probabilistic error cancellation. The proposed characterization procedure opens up possibilities for device calibration, hardware development, and improvement of error mitigation and correction techniques.},
	number = {{arXiv}:2307.08741},
	publisher = {{arXiv}},
	author = {Kaufmann, Noah and Rojkov, Ivan and Reiter, Florentin},
	urldate = {2024-05-03},
	date = {2023-07-17},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2307.08741 [quant-ph]},
	keywords = {Quantum Physics},
}

@article{kraus_general_1971,
	title = {General State Changes in Quantum Theory},
	volume = {64},
	issn = {0003-4916},
	url = {https://www.sciencedirect.com/science/article/pii/0003491671901084},
	doi = {10.1016/0003-4916(71)90108-4},
	abstract = {General state changes of quantum systems (operations) due to external interventions (measurements) are studied. The formalism developed in previous papers is simplified and generalized to systems with superselection rules. It describes a rather general class of linear mappings of states (density matrices). The set of operations is shown to be a semigroup which acts transitively on the set of all states. Applications of the formalism (local quantum field theory, localization of massless particles) are discussed shortly.},
	pages = {311--335},
	number = {2},
	journaltitle = {Annals of Physics},
	shortjournal = {Annals of Physics},
	author = {Kraus, K},
	urldate = {2024-05-03},
	date = {1971-06-01},
}

@book{breuer_theory_2007,
	title = {The Theory of Open Quantum Systems},
	isbn = {978-0-19-170634-9},
	url = {https://academic.oup.com/book/27757},
	abstract = {Abstract. This book treats the central physical concepts and mathematical techniques used to investigate the dynamics of open quantum systems. To provide a},
	publisher = {Oxford University Press},
	author = {Breuer, Heinz-Peter and Petruccione, Francesco},
	urldate = {2024-05-03},
	date = {2007-01-25},
	langid = {english},
	doi = {10.1093/acprof:oso/9780199213900.001.0001},
	doi = {10.1093/acprof:oso/9780199213900.001.0001},
}

@article{wang_overview_2015,
	title = {An Overview of Kernel Alignment and its Applications},
	volume = {43},
	issn = {1573-7462},
	url = {https://doi.org/10.1007/s10462-012-9369-4},
	doi = {10.1007/s10462-012-9369-4},
	abstract = {The success of kernel methods is very much dependent on the choice of kernel. Kernel design and learning a kernel from the data require evaluation measures to assess the quality of the kernel. In recent years, the notion of kernel alignment, which measures the degree of agreement between a kernel and a learning task, is widely used for kernel selection due to its effectiveness and low computational complexity. In this paper, we present an overview of the research progress of kernel alignment and its applications. We introduce the basic idea of kernel alignment and its theoretical properties, as well as the extensions and improvements for specific learning problems. The typical applications, including kernel parameter tuning, multiple kernel learning, spectral kernel learning and feature selection and extraction, are reviewed in the context of classification framework. The relationship between kernel alignment and other evaluation measures is also explored. Finally, concluding remarks and future directions are presented.},
	pages = {179--192},
	number = {2},
	journaltitle = {Artificial Intelligence Review},
	shortjournal = {Artif Intell Rev},
	author = {Wang, Tinghua and Zhao, Dongyan and Tian, Shengfeng},
	urldate = {2024-05-10},
	date = {2015-02-01},
	langid = {english},
	keywords = {Kernel alignment, Kernel evaluation measure, Kernel method, Learning kernels, Model selection},
}

@article{hubregtsen_training_2022,
	title = {Training Quantum Embedding Kernels on Near-Term Quantum Computers},
	volume = {106},
	issn = {2469-9926, 2469-9934},
	url = {http://arxiv.org/abs/2105.02276},
	doi = {10.1103/PhysRevA.106.042431},
	abstract = {Kernel methods are a cornerstone of classical machine learning. The idea of using quantum computers to compute kernels has recently attracted attention. Quantum embedding kernels ({QEKs}) constructed by embedding data into the Hilbert space of a quantum computer are a particular quantum kernel technique that allows to gather insights into learning problems and that are particularly suitable for noisy intermediate-scale quantum devices. In this work, we first provide an accessible introduction to quantum embedding kernels and then analyze the practical issues arising when realizing them on a noisy near-term quantum computer. We focus on quantum embedding kernels with variational parameters. These variational parameters are optimized for a given dataset by increasing the kernel-target alignment, a heuristic connected to the achievable classification accuracy. We further show under which conditions noise from device imperfections influences the predicted kernel and provide a strategy to mitigate these detrimental effects which is tailored to quantum embedding kernels. We also address the influence of finite sampling and derive bounds that put guarantees on the quality of the kernel matrix. We illustrate our findings by numerical experiments and tests on actual hardware.},
	pages = {042431},
	number = {4},
	journaltitle = {Physical Review A},
	shortjournal = {Phys. Rev. A},
	author = {Hubregtsen, Thomas and Wierichs, David and Gil-Fuster, Elies and Derks, Peter-Jan H. S. and Faehrmann, Paul K. and Meyer, Johannes Jakob},
	urldate = {2024-05-10},
	date = {2022-10-20},
	eprinttype = {arxiv},
	eprint = {2105.02276 [quant-ph]},
	keywords = {Computer Science - Machine Learning, Quantum Physics},
}

@article{cerezo_variational_2021,
	title = {Variational Quantum Algorithms},
	volume = {3},
	issn = {2522-5820},
	url = {http://arxiv.org/abs/2012.09265},
	doi = {10.1038/s42254-021-00348-9},
	abstract = {Applications such as simulating complicated quantum systems or solving large-scale linear algebra problems are very challenging for classical computers due to the extremely high computational cost. Quantum computers promise a solution, although fault-tolerant quantum computers will likely not be available in the near future. Current quantum devices have serious constraints, including limited numbers of qubits and noise processes that limit circuit depth. Variational Quantum Algorithms ({VQAs}), which use a classical optimizer to train a parametrized quantum circuit, have emerged as a leading strategy to address these constraints. {VQAs} have now been proposed for essentially all applications that researchers have envisioned for quantum computers, and they appear to the best hope for obtaining quantum advantage. Nevertheless, challenges remain including the trainability, accuracy, and efficiency of {VQAs}. Here we overview the field of {VQAs}, discuss strategies to overcome their challenges, and highlight the exciting prospects for using them to obtain quantum advantage.},
	pages = {625--644},
	number = {9},
	journaltitle = {Nature Reviews Physics},
	shortjournal = {Nat Rev Phys},
	author = {Cerezo, M. and Arrasmith, Andrew and Babbush, Ryan and Benjamin, Simon C. and Endo, Suguru and Fujii, Keisuke and {McClean}, Jarrod R. and Mitarai, Kosuke and Yuan, Xiao and Cincio, Lukasz and Coles, Patrick J.},
	urldate = {2024-05-13},
	date = {2021-08-12},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2012.09265 [quant-ph, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Quantum Physics},
}

@article{biamonte_quantum_2017,
	title = {Quantum machine learning},
	volume = {549},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/nature23474},
	doi = {10.1038/nature23474},
	pages = {195--202},
	number = {7671},
	journaltitle = {Nature},
	shortjournal = {Nature},
	author = {Biamonte, Jacob and Wittek, Peter and Pancotti, Nicola and Rebentrost, Patrick and Wiebe, Nathan and Lloyd, Seth},
	urldate = {2024-05-13},
	date = {2017-09},
	langid = {english},
}

@misc{winderl_quantum_2023,
	title = {Quantum Neural Networks under Depolarization Noise: Exploring White-Box Attacks and Defenses},
	url = {http://arxiv.org/abs/2311.17458},
	shorttitle = {Quantum Neural Networks under Depolarization Noise},
	abstract = {Leveraging the unique properties of quantum mechanics, Quantum Machine Learning ({QML}) promises computational breakthroughs and enriched perspectives where traditional systems reach their boundaries. However, similarly to classical machine learning, {QML} is not immune to adversarial attacks. Quantum adversarial machine learning has become instrumental in highlighting the weak points of {QML} models when faced with adversarial crafted feature vectors. Diving deep into this domain, our exploration shines light on the interplay between depolarization noise and adversarial robustness. While previous results enhanced robustness from adversarial threats through depolarization noise, our findings paint a different picture. Interestingly, adding depolarization noise discontinued the effect of providing further robustness for a multi-class classification scenario. Consolidating our findings, we conducted experiments with a multi-class classifier adversarially trained on gate-based quantum simulators, further elucidating this unexpected behavior.},
	number = {{arXiv}:2311.17458},
	publisher = {{arXiv}},
	author = {Winderl, David and Franco, Nicola and Lorenz, Jeanette Miriam},
	urldate = {2024-05-13},
	date = {2023-12-21},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2311.17458 [quant-ph]},
	keywords = {Computer Science - Artificial Intelligence, Quantum Physics},
}

@article{schuld_circuit-centric_2020,
	title = {Circuit-Centric Quantum Classifiers},
	volume = {101},
	issn = {2469-9926, 2469-9934},
	url = {http://arxiv.org/abs/1804.00633},
	doi = {10.1103/PhysRevA.101.032308},
	abstract = {The current generation of quantum computing technologies call for quantum algorithms that require a limited number of qubits and quantum gates, and which are robust against errors. A suitable design approach are variational circuits where the parameters of gates are learnt, an approach that is particularly fruitful for applications in machine learning. In this paper, we propose a low-depth variational quantum algorithm for supervised learning. The input feature vectors are encoded into the amplitudes of a quantum system, and a quantum circuit of parametrised single and two-qubit gates together with a single-qubit measurement is used to classify the inputs. This circuit architecture ensures that the number of learnable parameters is poly-logarithmic in the input dimension. We propose a quantum-classical training scheme where the analytical gradients of the model can be estimated by running several slightly adapted versions of the variational circuit. We show with simulations that the circuit-centric quantum classifier performs well on standard classical benchmark datasets while requiring dramatically fewer parameters than other methods. We also evaluate sensitivity of the classification to state preparation and parameter noise, introduce a quantum version of dropout regularisation and provide a graphical representation of quantum gates as highly symmetric linear layers of a neural network.},
	pages = {032308},
	number = {3},
	journaltitle = {Physical Review A},
	shortjournal = {Phys. Rev. A},
	author = {Schuld, Maria and Bocharov, Alex and Svore, Krysta and Wiebe, Nathan},
	urldate = {2024-05-13},
	date = {2020-03-06},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1804.00633 [quant-ph]},
	keywords = {Quantum Physics},
}

@article{pedregosa_scikit-learn_2011,
	title = {Scikit-learn: Machine Learning in Python},
	volume = {12},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v12/pedregosa11a.html},
	shorttitle = {Scikit-learn},
	abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and {API} consistency. It has minimal dependencies and is distributed under the simplified {BSD} license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
	pages = {2825--2830},
	number = {85},
	journaltitle = {Journal of Machine Learning Research},
	author = {Pedregosa, Fabian and Varoquaux, Gaël and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, Édouard},
	urldate = {2024-05-14},
	date = {2011},
	file = {Full Text PDF:C\:\\Users\\erick\\Zotero\\storage\\J4PPQ5MI\\Pedregosa et al. - 2011 - Scikit-learn Machine Learning in Python.pdf:application/pdf},
}

@article{smith_using_1988,
	title = {Using the {ADAP} Learning Algorithm to Forecast the Onset of Diabetes Mellitus},
	issn = {0195-4210},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2245318/},
	abstract = {Neural networks or connectionist models for parallel processing are not new. However, a resurgence of interest in the past half decade has occurred. In part, this is related to a better understanding of what are now referred to as hidden nodes. These algorithms are considered to be of marked value in pattern recognition problems. Because of that, we tested the ability of an early neural network model, {ADAP}, to forecast the onset of diabetes mellitus in a high risk population of Pima Indians. The algorithm's performance was analyzed using standard measures for clinical tests: sensitivity, specificity, and a receiver operating characteristic curve. The crossover point for sensitivity and specificity is 0.76. We are currently further examining these methods by comparing the {ADAP} results with those obtained from logistic regression and linear perceptron models using precisely the same training and forecasting sets. A description of the algorithm is included.},
	pages = {261--265},
	journaltitle = {Proceedings of the Annual Symposium on Computer Application in Medical Care},
	shortjournal = {Proc Annu Symp Comput Appl Med Care},
	author = {Smith, Jack W. and Everhart, J.E. and Dickson, W.C. and Knowler, W.C. and Johannes, R.S.},
	urldate = {2024-05-15},
	date = {1988-11-09},
	pmid = {null},
	pmcid = {PMC2245318},
	file = {PubMed Central Full Text PDF:C\:\\Users\\erick\\Zotero\\storage\\3U8ISEFI\\Smith et al. - 1988 - Using the ADAP Learning Algorithm to Forecast the .pdf:application/pdf},
}

@inproceedings{street_nuclear_1993,
	location = {San Jose, {CA}},
	title = {Nuclear Feature Extraction for Breast Tumor Diagnosis},
	url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=1008972},
	doi = {10.1117/12.148698},
	abstract = {Interactive image processing techniques, along with a linear-programming-based inductive classifier, have been used to create a highly accurate system for diagnosis of breast tumors. A small fraction of a fine needle aspirate slide is selected and digitized. With an interactive interface, the user initializes active contour models, known as snakes, near the boundaries of a set of cell nuclei. The customized snakes are deformed to the exact shape of the nuclei. This allows for precise, automated analysis of nuclear size, shape and texture. Ten such features are computed for each nucleus, and the mean value, largest (or 'worst') value and standard error of each feature are found over the range of isolated cells. After 569 images were analyzed in this fashion, different combinations of features were tested to find those which best separate benign from malignant samples. Ten-fold cross-validation accuracy of 97\% was achieved using a single separating plane on three of the thirty features: mean texture, worst area and worst smoothness. This represents an improvement over the best diagnostic results in the medical literature. The system is currently in use at the University of Wisconsin Hospitals. The same feature set has also been utilized in the much more difficult task of predicting distant recurrence of malignancy in patients, resulting in an accuracy of 86\%.},
	eventtitle = {{IS}\&T/{SPIE}'s Symposium on Electronic Imaging: Science and Technology},
	pages = {861--870},
	author = {Street, W. N. and Wolberg, W. H. and Mangasarian, O. L.},
	editor = {Acharya, Raj S. and Goldgof, Dmitry B.},
	urldate = {2024-05-15},
	date = {1993-07-29},
}

@inproceedings{bottou_comparison_1994,
	title = {Comparison of Classifier Methods: a Case Study in Handwritten Digit Recognition},
	volume = {2},
	url = {https://ieeexplore.ieee.org/document/576879},
	doi = {10.1109/ICPR.1994.576879},
	shorttitle = {Comparison of classifier methods},
	abstract = {This paper compares the performance of several classifier algorithms on a standard database of handwritten digits. We consider not only raw accuracy, but also training time, recognition time, and memory requirements. When available, we report measurements of the fraction of patterns that must be rejected so that the remaining patterns have misclassification rates less than a given threshold.},
	eventtitle = {Proceedings of the 12th {IAPR} International Conference on Pattern Recognition, Vol. 3 - Conference C: Signal Processing (Cat. No.94CH3440-5)},
	pages = {77--82 vol.2},
	booktitle = {Proceedings of the 12th {IAPR} International Conference on Pattern Recognition, Vol. 3 - Conference C: Signal Processing (Cat. No.94CH3440-5)},
	author = {Bottou, L. and Cortes, C. and Denker, J.S. and Drucker, H. and Guyon, I. and Jackel, L.D. and {LeCun}, Y. and Muller, U.A. and Sackinger, E. and Simard, P. and Vapnik, V.},
	urldate = {2024-05-15},
	date = {1994-10},
	keywords = {Computer aided software engineering, Databases, Handwriting recognition, Laboratories, Machine learning, {NIST}, Pattern recognition, Testing, Time measurement, Training data},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\erick\\Zotero\\storage\\UEZ26VDH\\576879.html:text/html},
}

@article{fisher_use_1936,
	title = {The Use of Multiple Measurements in Taxonomic Problems},
	volume = {7},
	rights = {1936 Blackwell Publishing Ltd/University College London},
	issn = {2050-1439},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1469-1809.1936.tb02137.x},
	doi = {10.1111/j.1469-1809.1936.tb02137.x},
	abstract = {The articles published by the Annals of Eugenics (1925–1954) have been made available online as an historical archive intended for scholarly use. The work of eugenicists was often pervaded by prejudice against racial, ethnic and disabled groups. The online publication of this material for scholarly research purposes is not an endorsement of those views nor a promotion of eugenics in any way.},
	pages = {179--188},
	number = {2},
	journaltitle = {Annals of Eugenics},
	author = {Fisher, R. A.},
	urldate = {2024-05-16},
	date = {1936},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1469-1809.1936.tb02137.x},
	file = {Full Text PDF:C\:\\Users\\erick\\Zotero\\storage\\H77NPWVM\\Fisher - 1936 - The Use of Multiple Measurements in Taxonomic Prob.pdf:application/pdf;Snapshot:C\:\\Users\\erick\\Zotero\\storage\\MKX49ME6\\j.1469-1809.1936.tb02137.html:text/html},
}

@misc{mottonen_transformation_2004,
	title = {Transformation of Quantum States Using Uniformly Controlled Rotations},
	url = {http://arxiv.org/abs/quant-ph/0407010},
	abstract = {We consider a unitary transformation which maps any given state of an \$n\$-qubit quantum register into another one. This transformation has applications in the initialization of a quantum computer, and also in some quantum algorithms. Employing uniformly controlled rotations, we present a quantum circuit of \$2{\textasciicircum}\{n+2\}-4n-4\$ {CNOT} gates and \$2{\textasciicircum}\{n+2\}-5\$ one-qubit elementary rotations that effects the state transformation. The complexity of the circuit is noticeably lower than the previously published results. Moreover, we present an analytic expression for the rotation angles needed for the transformation.},
	number = {{arXiv}:quant-ph/0407010},
	publisher = {{arXiv}},
	author = {Mottonen, Mikko and Vartiainen, Juha J. and Bergholm, Ville and Salomaa, Martti M.},
	urldate = {2024-06-05},
	date = {2004-07-01},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {quant-ph/0407010},
	keywords = {Quantum Physics},
}

@misc{schuld_supervised_2021,
	title = {Supervised Quantum Machine Learning Models are Kernel Methods},
	url = {http://arxiv.org/abs/2101.11020},
	abstract = {With near-term quantum devices available and the race for fault-tolerant quantum computers in full swing, researchers became interested in the question of what happens if we replace a supervised machine learning model with a quantum circuit. While such "quantum models" are sometimes called "quantum neural networks", it has been repeatedly noted that their mathematical structure is actually much more closely related to kernel methods: they analyse data in high-dimensional Hilbert spaces to which we only have access through inner products revealed by measurements. This technical manuscript summarises and extends the idea of systematically rephrasing supervised quantum models as a kernel method. With this, a lot of near-term and fault-tolerant quantum models can be replaced by a general support vector machine whose kernel computes distances between data-encoding quantum states. Kernel-based training is then guaranteed to find better or equally good quantum models than variational circuit training. Overall, the kernel perspective of quantum machine learning tells us that the way that data is encoded into quantum states is the main ingredient that can potentially set quantum models apart from classical machine learning models.},
	number = {{arXiv}:2101.11020},
	publisher = {{arXiv}},
	author = {Schuld, Maria},
	urldate = {2024-06-12},
	date = {2021-04-17},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2101.11020 [quant-ph, stat]},
	keywords = {Statistics - Machine Learning, Quantum Physics},
}

@article{benedetti_parameterized_2019,
	title = {Parameterized quantum circuits as machine learning models},
	volume = {4},
	issn = {2058-9565},
	url = {https://dx.doi.org/10.1088/2058-9565/ab4eb5},
	doi = {10.1088/2058-9565/ab4eb5},
	abstract = {Hybrid quantum–classical systems make it possible to utilize existing quantum computers to their fullest extent. Within this framework, parameterized quantum circuits can be regarded as machine learning models with remarkable expressive power. This Review presents the components of these models and discusses their application to a variety of data-driven tasks, such as supervised learning and generative modeling. With an increasing number of experimental demonstrations carried out on actual quantum hardware and with software being actively developed, this rapidly growing field is poised to have a broad spectrum of real-world applications.},
	pages = {043001},
	number = {4},
	journaltitle = {Quantum Science and Technology},
	shortjournal = {Quantum Sci. Technol.},
	author = {Benedetti, Marcello and Lloyd, Erika and Sack, Stefan and Fiorentini, Mattia},
	urldate = {2024-06-12},
	date = {2019-11},
	langid = {english},
	note = {Publisher: {IOP} Publishing},
}

@article{schuld_effect_2021,
	title = {Effect of Data Encoding on the Expressive Power of Variational Quantum-Machine-Learning Models},
	volume = {103},
	issn = {2469-9926, 2469-9934},
	url = {https://link.aps.org/doi/10.1103/PhysRevA.103.032430},
	doi = {10.1103/PhysRevA.103.032430},
	pages = {032430},
	number = {3},
	journaltitle = {Physical Review A},
	shortjournal = {Phys. Rev. A},
	author = {Schuld, Maria and Sweke, Ryan and Meyer, Johannes Jakob},
	urldate = {2024-06-12},
	date = {2021-03-24},
	langid = {english},
}

@article{rebentrost_quantum_2014,
	title = {Quantum Support Vector Machine for Big Data Classification},
	volume = {113},
	rights = {http://link.aps.org/licenses/aps-default-license},
	issn = {0031-9007, 1079-7114},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.113.130503},
	doi = {10.1103/PhysRevLett.113.130503},
	pages = {130503},
	number = {13},
	journaltitle = {Physical Review Letters},
	shortjournal = {Phys. Rev. Lett.},
	author = {Rebentrost, Patrick and Mohseni, Masoud and Lloyd, Seth},
	urldate = {2024-06-12},
	date = {2014-09-25},
	langid = {english},
}

@article{vanschoren_openml_2014,
	title = {{OpenML}: Networked Science in Machine Learning},
	volume = {15},
	issn = {1931-0145, 1931-0153},
	url = {http://arxiv.org/abs/1407.7722},
	doi = {10.1145/2641190.2641198},
	shorttitle = {{OpenML}},
	abstract = {Many sciences have made signiﬁcant breakthroughs by adopting online tools that help organize, structure and mine information that is too detailed to be printed in journals. In this paper, we introduce {OpenML}, a place for machine learning researchers to share and organize data in ﬁne detail, so that they can work more eﬀectively, be more visible, and collaborate with others to tackle harder problems. We discuss how {OpenML} relates to other examples of networked science and what beneﬁts it brings for machine learning research, individual scientists, as well as students and practitioners.},
	pages = {49--60},
	number = {2},
	journaltitle = {{ACM} {SIGKDD} Explorations Newsletter},
	shortjournal = {{SIGKDD} Explor. Newsl.},
	author = {Vanschoren, Joaquin and van Rijn, Jan N. and Bischl, Bernd and Torgo, Luis},
	urldate = {2024-06-13},
	date = {2014-06-16},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1407.7722 [cs]},
	keywords = {Computer Science - Computers and Society, Computer Science - Machine Learning},
	file = {1407.pdf:C\:\\Users\\erick\\Zotero\\storage\\ARMXCW3S\\1407.pdf:application/pdf},
}

@misc{wendlinger_comparative_2024,
	title = {A Comparative Analysis of Adversarial Robustness for Quantum and Classical Machine Learning Models},
	url = {http://arxiv.org/abs/2404.16154},
	abstract = {Quantum machine learning ({QML}) continues to be an area of tremendous interest from research and industry. While {QML} models have been shown to be vulnerable to adversarial attacks much in the same manner as classical machine learning models, it is still largely unknown how to compare adversarial attacks on quantum versus classical models. In this paper, we show how to systematically investigate the similarities and differences in adversarial robustness of classical and quantum models using transfer attacks, perturbation patterns and Lipschitz bounds. More specifically, we focus on classification tasks on a handcrafted dataset that allows quantitative analysis for feature attribution. This enables us to get insight, both theoretically and experimentally, on the robustness of classification networks. We start by comparing typical {QML} model architectures such as amplitude and re-upload encoding circuits with variational parameters to a classical {ConvNet} architecture. Next, we introduce a classical approximation of {QML} circuits (originally obtained with Random Fourier Features sampling but adapted in this work to fit a trainable encoding) and evaluate this model, denoted Fourier network, in comparison to other architectures. Our findings show that this Fourier network can be seen as a “middle ground” on the quantum-classical boundary. While adversarial attacks successfully transfer across this boundary in both directions, we also show that regularization helps quantum networks to be more robust, which has direct impact on Lipschitz bounds and transfer attacks.},
	number = {{arXiv}:2404.16154},
	publisher = {{arXiv}},
	author = {Wendlinger, Maximilian and Tscharke, Kilian and Debus, Pascal},
	urldate = {2024-06-13},
	date = {2024-04-24},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2404.16154 [quant-ph]},
	keywords = {Computer Science - Machine Learning, Computer Science - Cryptography and Security, Quantum Physics},
	file = {Wendlinger et al. - 2024 - A Comparative Analysis of Adversarial Robustness f.pdf:C\:\\Users\\erick\\Zotero\\storage\\DEQ6TDUM\\Wendlinger et al. - 2024 - A Comparative Analysis of Adversarial Robustness f.pdf:application/pdf},
}

@article{du_quantum_2021,
	title = {Quantum noise protects quantum classifiers against adversaries},
	volume = {3},
	url = {https://link.aps.org/doi/10.1103/PhysRevResearch.3.023153},
	doi = {10.1103/PhysRevResearch.3.023153},
	abstract = {Noise in quantum information processing is often viewed as a disruptive and difficult-to-avoid feature, especially in near-term quantum technologies. However, noise has often played beneficial roles, from enhancing weak signals in stochastic resonance to protecting the privacy of data in differential privacy. It is then natural to ask: Can we harness the power of quantum noise that is beneficial to quantum computing? An important current direction for quantum computing is its application to machine learning, such as classification problems. One outstanding problem in machine learning for classification is its sensitivity to adversarial examples. These are small, undetectable perturbations from the original data where the perturbed data is completely misclassified in otherwise extremely accurate classifiers. They can also be considered as worst-case perturbations by unknown noise sources. We show that by taking advantage of depolarization noise in quantum circuits for classification, a robustness bound against adversaries can be derived where the robustness improves with increasing noise. This robustness property is intimately connected with an important security concept called differential privacy, which can be extended to quantum differential privacy. For the protection of quantum data, this quantum protocol can be used against the most general adversaries. Furthermore, we show how the robustness in the classical case can be sensitive to the details of the classification model, but in the quantum case the details of the classification model are absent, thus also providing a potential quantum advantage for classical data. This opens the opportunity to explore other ways in which quantum noise can be used in our favor, as well as identifying other ways quantum algorithms can be helpful in a way which is distinct from quantum speedups.},
	pages = {023153},
	number = {2},
	journaltitle = {Physical Review Research},
	shortjournal = {Phys. Rev. Res.},
	author = {Du, Yuxuan and Hsieh, Min-Hsiu and Liu, Tongliang and Tao, Dacheng and Liu, Nana},
	urldate = {2024-06-14},
	date = {2021-05-27},
	note = {Publisher: American Physical Society},
}

@article{drummond_c45_nodate,
	title = {C4.5, Class Imbalance, and Cost Sensitivity: Why Under-Sampling beats Over-Sampling},
	abstract = {This paper takes a new look at two sampling schemes commonly used to adapt machine learning algorithms to imbalanced classes and misclassiﬁcation costs. It uses a performance analysis technique called cost curves to explore the interaction of over and undersampling with the decision tree learner C4.5. C4.5 was chosen as, when combined with one of the sampling schemes, it is quickly becoming the community standard when evaluating new cost sensitive learning algorithms. This paper shows that using C4.5 with undersampling establishes a reasonable standard for algorithmic comparison. But it is recommended that the cheapest class classiﬁer be part of that standard as it can be better than under-sampling for relatively modest costs. Over-sampling, however, shows little sensitivity, there is often little diﬀerence in performance when misclassiﬁcation costs are changed.},
	author = {Drummond, Chris and Holte, Robert C},
	langid = {english},
	file = {Drummond and Holte - C4.5, Class Imbalance, and Cost Sensitivity Why U.pdf:C\:\\Users\\erick\\Zotero\\storage\\GZHJWQZD\\Drummond and Holte - C4.5, Class Imbalance, and Cost Sensitivity Why U.pdf:application/pdf},
}

@article{west_benchmarking_2023,
	title = {Benchmarking Adversarially Robust Quantum Machine Learning at Scale},
	volume = {5},
	issn = {2643-1564},
	url = {https://link.aps.org/doi/10.1103/PhysRevResearch.5.023186},
	doi = {10.1103/PhysRevResearch.5.023186},
	pages = {023186},
	number = {2},
	journaltitle = {Physical Review Research},
	shortjournal = {Phys. Rev. Research},
	author = {West, Maxwell T. and Erfani, Sarah M. and Leckie, Christopher and Sevior, Martin and Hollenberg, Lloyd C. L. and Usman, Muhammad},
	urldate = {2024-06-18},
	date = {2023-06-23},
	langid = {english},
}

@article{lu_quantum_2020,
	title = {Quantum adversarial machine learning},
	volume = {2},
	issn = {2643-1564},
	url = {https://link.aps.org/doi/10.1103/PhysRevResearch.2.033212},
	doi = {10.1103/PhysRevResearch.2.033212},
	pages = {033212},
	number = {3},
	journaltitle = {Physical Review Research},
	shortjournal = {Phys. Rev. Research},
	author = {Lu, Sirui and Duan, Lu-Ming and Deng, Dong-Ling},
	urldate = {2024-06-19},
	date = {2020-08-06},
	langid = {english},
}
